{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D6giqCEScfp"
      },
      "source": [
        "# STOR 566, Homework 2\n",
        "### Instructor: Yao Li\n",
        "### Keywords: SVM, MLP, CNN\n",
        "### Due date: Sep 18, 11:55pm\n",
        "### **Submission Instruction**\n",
        "\n",
        "- Please download this script and use it to answer the questions in the homework. \n",
        "- For submission, please include your code, code output and answers in the script and submit the ipynb file on sakai.\n",
        "- Please don't modify existing cells. But you can add cells between the exercise statements.\n",
        "- To make markdown, please switch the cell type to markdown (from code) - you can hit 'm' when you are in command mode - and use the markdown language. For a brief tutorial see: https://daringfireball.net/projects/markdown/syntax\n",
        "\n",
        "### **References:**\n",
        "\n",
        "- You can follow the setup instructions at [here](https://pytorch.org/get-started/locally/).\n",
        "- A useful tutorial on learning pytorch by examples at [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n",
        "- More illustrations of different optimizers could be found [here](https://ruder.io/optimizing-gradient-descent/).\n",
        "- Check Pytorch optimization methods at [here](https://pytorch.org/docs/stable/optim.html)\n",
        "- Check Pytorch data augmentation options at [here](https://pytorch.org/vision/main/transforms.html).\n",
        "\n",
        "\n",
        "### **Evaluation Metrics of Classifiers:**\n",
        "\n",
        "- Average loss of an epoch: \n",
        "    \\begin{align}\n",
        "\t  \\frac{1}{B}\\sum_{b=1}^B\\sum_{d=1}^{D_b}\\frac{loss(y_{bd}, f(x_{bd}))}{D_b}\n",
        "\t  \\end{align}\n",
        "    for each training epoch\n",
        "    \n",
        "    - $B$: the total number of batches\n",
        "    - $D_b$: the number of observations in $b$-th batch\n",
        "    - $f$: the model (Logistic regression or Linear SVM or MLP or CNN)\n",
        "    - loss: logistic loss or the loss of linear SVM or cross-entropy\n",
        "    - $(x_{bd}, y_{bd})$: the $d$-th pair of input data and label in $b$-th batch\n",
        "    - An epoch is defined as one iteration over all observations in the training dataset\n",
        "\n",
        "- Testing accuracy: \n",
        "\t\\begin{align}\n",
        "\t\\frac{1}{N}\\sum_{i=1}^N {\\bf 1}(\\hat{y}_i=y_i)\n",
        "\t\\end{align}\t\n",
        "    - $N$: the total number of samples in the testing set\n",
        "    - $y_i$: true label of sample $i$\n",
        "    - $\\hat{y}_i$: predicted label by the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGLwKI0AjmvW"
      },
      "source": [
        "# Mikhal Ben-Joseph\n",
        "I abided by the Honor Code while I completed this assignment. I worked together with Yesh Munagala and I utilized several internet resources which I listed along with Stack Overflow and the Pytorch documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB6khqRukUzQ"
      },
      "source": [
        "## Problem 1 (40 points)\n",
        "\n",
        "In this problem you will practice implementing Linear SVM and Logistic Regression to classify **handwritten digit 0 and 1**.\n",
        "\n",
        "**Data.** You will use MNIST digit classification dataset. Pytorch/torchvision has provide a useful dataloader to automatically download and load the data into batches. In this homework, you need two class, digit 0 and digit 1, for binary classification. Code of the data loader has been provided in the template. Please don't modify the data loading part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKxBocWvdBtA",
        "outputId": "93fbcf8e-fd45-4ebc-f757-9c93e2addc75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "## The following code can change the working directory to your google drive\n",
        "## So you don't need to download the data every time\n",
        "\n",
        "import os\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaUdejVu7hhp"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UFlhPdZWXDSs"
      },
      "outputs": [],
      "source": [
        "## Data loading code chunk, please don't modify it. \n",
        "## However, you can adjust the batch size if you want to.\n",
        "batch_size = 64\n",
        "data_dir = './data'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.utils.data as td\n",
        "import random, time\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "## USE THIS SNIPPET TO GET BINARY TRAIN/TEST DATA\n",
        "\n",
        "train_data = datasets.MNIST(data_dir, train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "# Once you have downloaded the data by setting download=True, you can\n",
        "# change download=True to download=False\n",
        "test_data = datasets.MNIST(data_dir, train=False, download=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "\n",
        "subset_indices = ((train_data.targets == 0) + (train_data.targets == 1)).nonzero()\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size, \n",
        "  shuffle=False,sampler=SubsetRandomSampler(subset_indices.view(-1)))\n",
        "\n",
        "\n",
        "subset_indices = ((test_data.targets == 0) + (test_data.targets == 1)).nonzero()\n",
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size, \n",
        "  shuffle=False,sampler=SubsetRandomSampler(subset_indices.view(-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_VY0XmUkteW"
      },
      "source": [
        "### **Problem Description.**\n",
        "### 1. (20 points) Implement **Logistic Regression** with Pytorch to do handwritten digit 0 vs. 1 classification. Pick an optimizer yourself. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8_fM5kfXU6hq"
      },
      "outputs": [],
      "source": [
        "## Implementation of Logistic Regression\n",
        "## You can insert more code chunks and text cells if you want to.\n",
        "## Your code:\n",
        "\n",
        "## A template is provided but you don't have to follow it:\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "class LogisticLoss(torch.nn.Module):\n",
        "\n",
        " def __init__(self):\n",
        "   super(LogisticLoss, self).__init__()\n",
        "\n",
        " def forward(self, y_predict, labels):\n",
        "   # Minji told me in OH that I can write the loss function myself instead of using the sigmoid\n",
        "   response = (torch.log(1+torch.exp(-labels * y_predict.t())))\n",
        "   response = torch.mean(response)\n",
        "   return response\n",
        "\n",
        "# Training the Model\n",
        "\n",
        "def hype_tune_logistic(num_epochs, lr, batch_size):\n",
        "  print(\"\")\n",
        "  print(\" >>>> Number of Epochs: \", num_epochs, \" Learning Rate: \", lr, \" Batch Size: \", batch_size, \"<<<<\")\n",
        "  print(\"\")\n",
        "\n",
        "  ### Intantiating the model\n",
        "  model = nn.Linear(28*28, 1)\n",
        "  criterion = LogisticLoss()\n",
        "  iter = 0\n",
        "  losses = list()\n",
        "  optimizer = optim.Adagrad(model.parameters(),lr=lr)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      epoch_losses = list()\n",
        "\n",
        "      # for i, (images, labels) in enumerate(train_loader):\n",
        "      for (images, labels) in (train_loader):\n",
        "\n",
        "          # Convert the 28*28 image matrix into a 784-dim vector\n",
        "          images = images.view(-1, 28*28) \n",
        "          # Convert labels from 0,1 to -1,1\n",
        "          labels = 2*(labels.float()-0.5)\n",
        "          \n",
        "          ## TODO \n",
        "\n",
        "          # 1. Compute Loss. Check torch functions for the corresponding loss for Logistic and SVM\n",
        "          optimizer.zero_grad()\n",
        "          y_predict = model(images)\n",
        "          loss = criterion(y_predict, labels)\n",
        "          \n",
        "          # 2. Do optimization. Chec torch.optim to see how to do optimization with pytorch\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          iter += 1\n",
        "\n",
        "          # 3. Save batch loss\n",
        "          epoch_losses.append(loss.item()) \n",
        "\n",
        "      ## Save average epoch loss\n",
        "      losses.append(np.average(epoch_losses))\n",
        "      print('Epoch: {} :: Iteration: {} :: Loss: {}  '.format(epoch, iter, losses[-1], ))\n",
        "\n",
        "  return losses, model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkkph72gNqHS",
        "outputId": "540b6a8b-59f7-400f-b32c-31fb65d2b96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.01  Batch Size:  64 <<<<\n",
            "\n",
            "Epoch: 0 :: Iteration: 198 :: Loss: 0.023091642598790877  \n",
            "Epoch: 1 :: Iteration: 396 :: Loss: 0.0074939156142816024  \n",
            "Epoch: 2 :: Iteration: 594 :: Loss: 0.005719621877439998  \n",
            "Epoch: 3 :: Iteration: 792 :: Loss: 0.004852813125571537  \n",
            "Epoch: 4 :: Iteration: 990 :: Loss: 0.004293946226214463  \n",
            "Epoch: 5 :: Iteration: 1188 :: Loss: 0.0038194294130803564  \n",
            "Epoch: 6 :: Iteration: 1386 :: Loss: 0.003588828573801856  \n",
            "Epoch: 7 :: Iteration: 1584 :: Loss: 0.0033589625821361844  \n",
            "Epoch: 8 :: Iteration: 1782 :: Loss: 0.0031562354955104923  \n",
            "Epoch: 9 :: Iteration: 1980 :: Loss: 0.0030132865016860183  \n",
            "\n",
            " >>>> Number of Epochs:  15  Learning Rate:  0.01  Batch Size:  64 <<<<\n",
            "\n",
            "Epoch: 0 :: Iteration: 198 :: Loss: 0.026095893667926164  \n",
            "Epoch: 1 :: Iteration: 396 :: Loss: 0.008061100201413148  \n",
            "Epoch: 2 :: Iteration: 594 :: Loss: 0.006016597866123034  \n",
            "Epoch: 3 :: Iteration: 792 :: Loss: 0.004876009569588033  \n",
            "Epoch: 4 :: Iteration: 990 :: Loss: 0.004268646210983555  \n",
            "Epoch: 5 :: Iteration: 1188 :: Loss: 0.00398294900110664  \n",
            "Epoch: 6 :: Iteration: 1386 :: Loss: 0.003609685191262579  \n",
            "Epoch: 7 :: Iteration: 1584 :: Loss: 0.0033418991099666766  \n",
            "Epoch: 8 :: Iteration: 1782 :: Loss: 0.00317495229977096  \n",
            "Epoch: 9 :: Iteration: 1980 :: Loss: 0.003011011302684589  \n",
            "Epoch: 10 :: Iteration: 2178 :: Loss: 0.002867503657600739  \n",
            "Epoch: 11 :: Iteration: 2376 :: Loss: 0.002759630660508228  \n",
            "Epoch: 12 :: Iteration: 2574 :: Loss: 0.0026491910954578686  \n",
            "Epoch: 13 :: Iteration: 2772 :: Loss: 0.0025551167629527475  \n",
            "Epoch: 14 :: Iteration: 2970 :: Loss: 0.0024544326847212184  \n",
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  64 <<<<\n",
            "\n",
            "Epoch: 0 :: Iteration: 198 :: Loss: 0.03072657256615484  \n",
            "Epoch: 1 :: Iteration: 396 :: Loss: 0.005808264970161542  \n",
            "Epoch: 2 :: Iteration: 594 :: Loss: 0.003497235110411793  \n",
            "Epoch: 3 :: Iteration: 792 :: Loss: 0.0024414266817807386  \n",
            "Epoch: 4 :: Iteration: 990 :: Loss: 0.002022762229673374  \n",
            "Epoch: 5 :: Iteration: 1188 :: Loss: 0.001715142630266808  \n",
            "Epoch: 6 :: Iteration: 1386 :: Loss: 0.0013166229435972903  \n",
            "Epoch: 7 :: Iteration: 1584 :: Loss: 0.0012000485863045802  \n",
            "Epoch: 8 :: Iteration: 1782 :: Loss: 0.0009625612925298324  \n",
            "Epoch: 9 :: Iteration: 1980 :: Loss: 0.0007738221267903446  \n",
            "\n",
            " >>>> Number of Epochs:  15  Learning Rate:  0.1  Batch Size:  64 <<<<\n",
            "\n",
            "Epoch: 0 :: Iteration: 198 :: Loss: 0.016680246918098683  \n",
            "Epoch: 1 :: Iteration: 396 :: Loss: 0.0035796624455994358  \n",
            "Epoch: 2 :: Iteration: 594 :: Loss: 0.0023940328477746646  \n",
            "Epoch: 3 :: Iteration: 792 :: Loss: 0.0017587671333614589  \n",
            "Epoch: 4 :: Iteration: 990 :: Loss: 0.0009692560754992676  \n",
            "Epoch: 5 :: Iteration: 1188 :: Loss: 0.0007894544850825272  \n",
            "Epoch: 6 :: Iteration: 1386 :: Loss: 0.0009198808166422967  \n",
            "Epoch: 7 :: Iteration: 1584 :: Loss: 0.0007325678894858356  \n",
            "Epoch: 8 :: Iteration: 1782 :: Loss: 0.000565070167039441  \n",
            "Epoch: 9 :: Iteration: 1980 :: Loss: 0.00043740654908994907  \n",
            "Epoch: 10 :: Iteration: 2178 :: Loss: 0.00047780453267488485  \n",
            "Epoch: 11 :: Iteration: 2376 :: Loss: 0.00032009438711377174  \n",
            "Epoch: 12 :: Iteration: 2574 :: Loss: 0.0003246146603710499  \n",
            "Epoch: 13 :: Iteration: 2772 :: Loss: 0.0003502919559847132  \n",
            "Epoch: 14 :: Iteration: 2970 :: Loss: 0.000329216213004321  \n"
          ]
        }
      ],
      "source": [
        "### I chose SGD as the first optimizer to try because it is the most basic of the ones we learned\n",
        "### It didn't work too well so I'm trying Adagrad instead\n",
        "\n",
        "loss1, model1 = hype_tune_logistic(10, 0.01, 64)\n",
        "loss2, model2 = hype_tune_logistic(15, 0.01, 64)\n",
        "loss3, model3 = hype_tune_logistic(10, 0.1, 64)\n",
        "loss4, model4 = hype_tune_logistic(15, 0.1, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7FujWs1Vk1G"
      },
      "source": [
        "Note: When I first tried to implement Logistic Regression, I used the sigmoid function in an activation layer as detailed in the code chunk below. After I spoke with Minji in Office Hours, she suggested I tried writing my own loss function. I received different answers when using the method below, so I have decided to stick with the version above where I wrote my own loss function. The commented code below is **not my answer**, but I wanted to keep my code here so that in the future I can come back and find what went wrong between the two. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jsD4haidWeDq"
      },
      "outputs": [],
      "source": [
        "# ## Implementation of Logistic Regression\n",
        "# ## You can insert more code chunks and text cells if you want to.\n",
        "# ## Your code:\n",
        "\n",
        "# ## A template is provided but you don't have to follow it:\n",
        "\n",
        "# # The number of epochs is at least 10, you can increase it to achieve better performance\n",
        "# num_epochs = 10\n",
        "\n",
        "# # Here is my model definition:\n",
        "# class Logistic_Regression(torch.nn.Module):\n",
        "\n",
        "#  def __init__(self,num_input, num_output):\n",
        "#    super(Logistic_Regression,self).__init__()\n",
        "#    ### I am adding in one linear layer that will get fed to the activation\n",
        "#    self.layer1=torch.nn.Linear(num_input, num_output)\n",
        "\n",
        "#  def forward(self,x):\n",
        "#    y_predict = torch.sigmoid(self.layer1(x))\n",
        "#    return y_predict\n",
        "\n",
        "\n",
        "# # Training the Model\n",
        "\n",
        "# def hype_tune_logistic(num_epochs, lr, batch_size):\n",
        "#   print(\"\")\n",
        "#   print(\" >>>> Number of Epochs: \", num_epochs, \" Learning Rate: \", lr, \" Batch Size: \", batch_size, \"<<<<\")\n",
        "#   print(\"\")\n",
        "\n",
        "#   ### Intantiating the model\n",
        "#   model = Logistic_Regression(28*28, 1)\n",
        "\n",
        "#   ### I am going to use the Binary Cross Entropy Loss for Logistic\n",
        "#   criterion = nn.BCELoss()\n",
        "\n",
        "#   iter = 0\n",
        "#   losses = list()\n",
        "#   optimizer = optim.Adagrad(model.parameters(),lr=lr)\n",
        "\n",
        "#   for epoch in range(num_epochs):\n",
        "\n",
        "#       epoch_losses = list()\n",
        "\n",
        "#       # for i, (images, labels) in enumerate(train_loader):\n",
        "#       for (images, labels) in (train_loader):\n",
        "\n",
        "#           # Convert the 28*28 image matrix into a 784-dim vector\n",
        "#           images = images.view(-1, 28*28) \n",
        "#           # Convert labels from 0,1 to -1,1 is not necessary because we are using BCE\n",
        "#           #labels = 2*(labels.float()-0.5)\n",
        "#           labels = labels.float()\n",
        "          \n",
        "#           ## TODO \n",
        "\n",
        "#           # 1. Compute Loss. Check torch functions for the corresponding loss for Logistic and SVM\n",
        "        \n",
        "#           y_predict = model(images)\n",
        "#           labels = labels.unsqueeze(1)\n",
        "#           loss = criterion(y_predict, labels)\n",
        "          \n",
        "#           # 2. Do optimization. Chec torch.optim to see how to do optimization with pytorch\n",
        "#           optimizer.zero_grad()\n",
        "#           loss.backward()\n",
        "#           optimizer.step()\n",
        "          \n",
        "#           iter += 1\n",
        "\n",
        "#           # 3. Save batch loss\n",
        "#           epoch_losses.append(loss.item()) \n",
        "\n",
        "#       ## Save average epoch loss\n",
        "#         # if (iter) % 64 == 0:\n",
        "#       losses.append(np.average(epoch_losses))\n",
        "#       print('Epoch: {} :: Iteration: {} :: Loss: {}  '.format(epoch, iter, losses[-1], ))\n",
        "\n",
        "#   return losses, model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkUY4Y8mPGCf"
      },
      "source": [
        "Please note that I used the following website resources while constructing my code for logistic regression:\n",
        "\n",
        " https://www.analyticsvidhya.com/blog/2021/07/perform-logistic-regression-with-pytorch-seamlessly/\n",
        "\n",
        "https://towardsdatascience.com/logistic-regression-with-pytorch-3c8bbea594be\n",
        "\n",
        "https://neptune.ai/blog/pytorch-loss-functions\n",
        "\n",
        "https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_logistic_regression/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UZxum1LZSD8"
      },
      "source": [
        "### (a) (5 points) Report the hyper-parameters (number of epochs, learning rate, momentum etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emZ2QD136RvQ",
        "outputId": "cc96051b-d3bb-4f01-dca8-432577775d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.000329216213004321\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "list_of_losses = [loss1[-1], loss2[-1], loss3[-1], loss4[-1]]\n",
        "min_loss = min(list_of_losses)\n",
        "print(min_loss)\n",
        "min_params = list_of_losses.index(min_loss)\n",
        "print(min_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kEgec89ZT5W"
      },
      "source": [
        "Answer: I arrived at the smallest final loss when I used 15 epochs, a learning rate of 0.1, and a batch size of 64 (AKA mod4). Since I was using Adagrad as my optimizer, there was no momentum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tigk4RBkyFb"
      },
      "source": [
        "### (b) (10 points) Report the **Average loss of an epoch** for every epoch by generating Average Loss vs. Epoch plot. Please report at least **10** epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "S-IUsPlvYEGJ",
        "outputId": "90b561dc-305e-4448-bd15-98a65f9f20ef"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxddZ3/8dc7a3vTNWko3ZJUKWhF2TqIwqgjLkWRMooCiqLyGH7OgPv8RvjN6PhDndH56aAO6FgFQRQBOy4dhxEREIdFoCwuZZECpRul6b6kbZrk8/vjfFMuIctNm5ubm7yfj8flnvM933Pu54Q0n3u+3+85X0UEZmZmhaoodQBmZlZenDjMzGxQnDjMzGxQnDjMzGxQnDjMzGxQnDjMzGxQnDjMrCQkvU7SmlLHYYPnxGHDStKvJW2RVFvqWA6WpM9K+n6p4xgKklokhaSdPV5nljo2G3mqSh2AjR2SWoA/B7YBpwE/GuLjV0VEx1Aecwya4p+hDcRXHDac3gf8FrgKOBdAUq2krZKO7K4kqVHSbkmHpPVTJT2U6t0l6RV5dVdK+pSk3wO7JFVJukjSE5J2SHpY0l/m1a+U9BVJGyU9JenC9E27Km2fLOkKSc9IWivp85IqB3uikk6TtDzF/GtJL83b9ql07B2SHpN0cio/XtIySdslPSvpX/s49iOSTs1br5LUKulYSeMkfV/SpvTZ90maPtj4e/nMqyT9u6SbU9y3S2rO2/7q9Fnb0vur87bVS/qupHXpavOnPY79SUkb0s/8Awcbqw2DiPDLr2F5ASuAvwGOA/YB01P5lcAX8updAPwiLR8DbABeCVSSJZyVQG3avhJ4CJgDjE9l7wRmkn0xOhPYBcxI2z4EPAzMBqYCvwICqErbfwJ8C6gDDgHuBf5XH+fzWeD7vZQfnj7zjUA18Hfp3GuAI4DVwMxUtwV4cVq+G3hvWp4AnNDH534G+EHe+luBR9Ly/wL+E8iln9dxwKQC/t+05P8cetl+FbADeA1QC3wNuCNtqwe2AO8la8U4O603pO3/BVyfft7VwGtT+euADuCSVP4WoA2YWurfVb8G+H0pdQB+jY0XcFJKFtPS+qPAx9PyG4An8ureCbwvLX8T+FyPYz2W98dnJfDBAT77IWBRWr41PxGkz470B286sJeUgNL2s4Hb+jhuX4nj08ANeesVwNr0h/IwskT4BqC6x36/Af5v98+on/M5LP0Rz6X1HwCfScsfBO4CXjHI/z/diWNrj9dL0/argOvy6k8AOskS9nuBe3sc727g/cAMoKu3ZJB+HrvJS1bpZ9NrwvRr5LzcVGXD5VzglxGxMa1fm8oAbgNykl6Z+kGOJvvmD9AMfDI1u2yVtJXsj9XMvGOvzv8gSe/La9raChwJTEubZ/aon7/cTPbN95m8fb9FduUxGDOBp7tXIqIrfc6siFgBfIws6WyQdJ2k7nM5j+xq5dHU3HMqvUjHeAR4m6QcWX/RtWnzNcBNwHWpaehfJFUPIvZpETEl7/VI3rb9P6uI2AlsTuf6vPNNngZmkf2/2hwRW/r4vE3x/D6VNrKkZCOYO8et6CSNB94FVEpan4prgSmSjoqI30m6gezb/bPAzyNiR6q3mqwZ6wv9fMT+RzyndvdvAycDd0dEp6SHAKUqz5A1U3Wbk7e8muyKY1ocXAfxOuDleTEpfc5agIi4FrhW0iSyxPQlsiaqx4GzJVUAbweWSGqIiF29fMYPyX5eFcDDKZkQEfvIrlr+b0rCN5JdoV1xEOfTbf/PStIEsiaqdenV3KNuE/ALsp9pvaQpEbF1CGKwEcBXHDYcTidr1phPdjVxNPBS4H/IOswh+8Z8JvAenvv2DFkS+FC6GpGkOklvlTSxj8+qI0skrQCps/XIvO03AB+VNEvSFOBT3Rsi4hngl8BXJE2SVCHpxZJe28+5VaQO6e5XbfqMt0o6OX3b/yRZQrpL0hGSXp/q7SFrqulKsZ4jqTFdoXT/ke3q43OvA94E/HX+z0vSX0h6eerQ307WPNjXMQbrLZJOklQDfA74bUSsJktOh0t6d+qoP5Ps//XP08/0v4FvSJoqqVrSa4YoHisRJw4bDucC342IVRGxvvsFXAa8R9kw2nvIOpRnkv2hASAilgF/lepuIetkfn9fHxQRDwNfIWtjf5bsm/+deVW+TZYcfg88SPZHr4MssUGWyGrIOtC3AEvI2un7cjbZH//u1xMR8RhwDvBvwEbgbcDbIqKd7Erri6l8PVkz2MXpWAuB5ZJ2knU+nxURu/s4z2fSOb6arOO526Ep5u1kzVm3kzVfkUZF/Xs/5wKwVc+/j+MTeduuBf6RrInquHSORMQm4FSyBLmJbDDAqXnNku8lS2CPkvVhfGyAGGyEU4QncrKxS9IpwL9HRM+mFssj6SpgTUT8Q6ljsdLzFYeNKZLGS3pLalKZRfYN+icD7Wdmz3HisLFGZJ3HW8iaqh4huy/CzArkpiozMxsUX3GYmdmgjIn7OKZNmxYtLS2lDsPMrKzcf//9GyOisWf5mEgcLS0tLFu2rNRhmJmVFUk9nwgAuKnKzMwGyYnDzMwGxYnDzMwGxYnDzMwGxYnDzMwGxYnDzMwGxYnDzMwGxYmjH9+7eyVLf7eu1GGYmY0oThz9uP6+1fz4gTWlDsPMbERx4uhHc0OOVZvaSh2GmdmI4sTRj6b6OlZvaaOzy08QNjPr5sTRj+aGHPs6g2e29Tp7p5nZmOTE0Y/m+hyAm6vMzPIUNXFIWijpMUkrJF3Uy/ZaSden7fdIaknlDZJuk7RT0mU99qmRtFjSnyQ9KukdxYq/qSFLHE9vduIwM+tWtMeqS6oELgfeCKwB7pO0NCIezqt2HrAlIg6TdBbwJeBMYA/waeDI9Mr398CGiDhcUgVQX6xzmDF5PDWVFazctKtYH2FmVnaKecVxPLAiIp6MiHbgOmBRjzqLgKvT8hLgZEmKiF0RcQdZAunpg8A/A0REV0RsLE74UFkhZtePd1OVmVmeYiaOWcDqvPU1qazXOhHRAWwDGvo6oKQpafFzkh6Q9CNJ0/uoe76kZZKWtba2Hug50Fyf42knDjOz/cqtc7wKmA3cFRHHAncDX+6tYkQsjogFEbGgsfEFMx8WrLmhjlWb24jwkFwzMyhu4lgLzMlbn53Keq0jqQqYDGzq55ibgDbgx2n9R8CxQxFsX5rqc+zc28HmXe3F/Bgzs7JRzMRxHzBP0lxJNcBZwNIedZYC56blM4Bbo5+v9mnbfwKvS0UnAw/3VX8oNHtklZnZ8xRtVFVEdEi6ELgJqASujIjlki4BlkXEUuAK4BpJK4DNZMkFAEkrgUlAjaTTgTelEVmfSvt8FWgFPlCsc4CsqQrg6U27OLZpajE/ysysLBQtcQBExI3AjT3KPpO3vAd4Zx/7tvRR/jTwmqGLsn9z6scj4Q5yM7Ok3DrHh11tVSUzJo3zkFwzs8SJowBNDTn3cZiZJU4cBWiur3NTlZlZ4sRRgKaGHBt37mXX3o5Sh2JmVnJOHAXYPyTXVx1mZk4chWhJQ3JXbfbDDs3MnDgK0OQrDjOz/Zw4CjBpXDVTc9UeWWVmhhNHwZoa6nwvh5kZThwFa67P8bT7OMzMnDgK1dyQY+2W3bR3dJU6FDOzknLiKFBzQx1dAWu37i51KGZmJeXEUaDn7uVwc5WZjW1OHAVqrs8SxyqPrDKzMc6Jo0CNE2sZX13peznMbMwrauKQtFDSY5JWSLqol+21kq5P2++R1JLKGyTdJmmnpMv6OPZSSX8sZvw9Po+m+pwTh5mNeUVLHJIqgcuBU4D5wNmS5veodh6wJSIOAy4FvpTK9wCfBv62j2O/HdhZjLj709SQcx+HmY15xbziOB5YERFPRkQ7cB2wqEedRcDVaXkJcLIkRcSuiLiDLIE8j6QJwCeAzxcv9N61NORYtbmNrq4+p0U3Mxv1ipk4ZgGr89bXpLJe60REB7ANaBjguJ8DvgIMe5tRU0Mdezu62LBj73B/tJnZiFFWneOSjgZeHBE/KaDu+ZKWSVrW2to6JJ/fPbLKzVVmNpYVM3GsBebkrc9OZb3WkVQFTAY29XPMVwELJK0E7gAOl/Tr3ipGxOKIWBARCxobGw/oBHrafy+Hh+Sa2RhWzMRxHzBP0lxJNcBZwNIedZYC56blM4BbI6LPDoSI+GZEzIyIFuAk4E8R8bohj7wPM6eMp7JCftihmY1pVcU6cER0SLoQuAmoBK6MiOWSLgGWRcRS4ArgGkkrgM1kyQWAdFUxCaiRdDrwpoh4uFjxFqK6soJZU8b7isPMxrSiJQ6AiLgRuLFH2WfylvcA7+xj35YBjr0SOPKggxykZg/JNbMxrqw6x0eCLHH4isPMxi4njkFqrq9j2+59bGvbV+pQzMxKwoljkPbPP+5JncxsjHLiGKTnHq/u5iozG5ucOAapyY9XN7MxzoljkHI1VTROrGXlRjdVmdnY5MRxAFoacr6Xw8zGLCeOA9BUX+e7x81szHLiOADNDTnWb9/Dnn2dpQ7FzGzYOXEcgO6RVavdXGVmY5ATxwFoqveQXDMbu5w4DkBzQx0AK/3MKjMbg5w4DsDUXDUTa6t8L4eZjUlOHAdAEs3T/LBDMxubnDgOUHN9na84zGxMcuI4QE0NOdZsaaOzq88JC83MRqWiJg5JCyU9JmmFpIt62V4r6fq0/R5JLam8QdJtknZKuiyvfk7Sf0l6VNJySV8sZvz9aa7Psa8zWLd1d6lCMDMriaIlDkmVwOXAKcB84GxJ83tUOw/YEhGHAZcCX0rle4BPA3/by6G/HBEvAY4BTpR0SjHiH0iTn5JrZmNUMa84jgdWRMSTEdEOXAcs6lFnEXB1Wl4CnCxJEbErIu4gSyD7RURbRNyWltuBB4DZRTyHPnUPyfW8HGY21hQzccwCVuetr0llvdaJiA5gG9BQyMElTQHeBtzSx/bzJS2TtKy1tXWQoQ9sxqRx1FRV+JlVZjbmlGXnuKQq4IfA1yPiyd7qRMTiiFgQEQsaGxuHPIaKCjFn6ng3VZnZmFPMxLEWmJO3PjuV9VonJYPJwKYCjr0YeDwivjoEcR6w5oY6P17dzMacYiaO+4B5kuZKqgHOApb2qLMUODctnwHcGhH9jm+V9HmyBPOxIY530Jrqc6zatIsBQjYzG1WqinXgiOiQdCFwE1AJXBkRyyVdAiyLiKXAFcA1klYAm8mSCwCSVgKTgBpJpwNvArYDfw88CjwgCeCyiPhOsc6jP80NOXa1d7JxZzuNE2tLEYKZ2bArWuIAiIgbgRt7lH0mb3kP8M4+9m3p47AaqvgOVvfj1Vdt3uXEYWZjRll2jo8U+4fkuoPczMYQJ46DMHvqeCQnDjMbW5w4DkJtVSUzJ4/3ww7NbExx4jhITfU5nvaETmY2hjhxHKTmhpyvOMxsTBkwcUh6saTatPw6SR9Jj/swsocdbtzZzs69HaUOxcxsWBRyxfEfQKekw8ju2J4DXFvUqMpIc333yCo3V5nZ2FBI4uhKDyD8S+DfIuJ/AzOKG1b52H8vh0dWmdkYUUji2CfpbLJHg/w8lVUXL6Ty0p04/MwqMxsrCkkcHwBeBXwhIp6SNBe4prhhlY+J46qpr6vxvRxmNmYM+MiRiHgY+AiApKnAxIj4Uv97jS1N9TlWeUInMxsjChlV9WtJkyTVk824921J/1r80MpHc0OOlRt9xWFmY0MhTVWTI2I78HbgexHxSuANxQ2rvDTX53hm227aO7pKHYqZWdEVkjiqJM0A3sVzneOWp7mhjq6ANVt81WFmo18hieMSsjk1noiI+yS9CHi8uGGVF4+sMrOxZMDEERE/iohXRMRfp/UnI+IdhRxc0kJJj0laIemiXrbXSro+bb9HUksqb5B0m6Sdki7rsc9xkv6Q9vm60mxOpdTkeznMbAwppHN8tqSfSNqQXv8haXYB+1UClwOnAPOBsyXN71HtPGBLRBwGXAp0j9baA3wa+NteDv1N4K+Aeem1cKBYiq1xQi25mkoPyTWzMaGQpqrvks0NPjO9/jOVDeR4YEW6QmkHrgMW9aizCLg6LS8BTpakiNgVEXeQJZD9Ul/LpIj4bZqb/HvA6QXEUlSS/JRcMxszCkkcjRHx3YjoSK+rgMYC9psFrM5bX5PKeq2THmuyDWgY4JhrBjgmAJLOl7RM0rLW1tYCwj04TfU593GY2ZhQSOLYJOkcSZXpdQ6wqdiBHayIWBwRCyJiQWNjIXnu4LRMq2PV5ja6uqLon2VmVkqFJI4Pkg3FXQ88A5wBvL+A/daSPUm32+xU1msdSVXAZPpPSmvTcfo7Zkk01edo7+ji2R17Bq5sZlbGChlV9XREnBYRjRFxSEScDny0gGPfB8yTNFdSDXAWWV9JvqVkD0+ELCHdmvou+orlGWC7pBPSaKr3AT8rIJai2z8k1x3kZjbKHegMgO8aqELqs7iQ7B6QR4AbImK5pEsknZaqXQE0SFoBfALYP2RX0krgX4H3S1qTNyLrb4DvACuAJ4D/PsBzGFLd83J4SK6ZjXYDPuSwDwXdOxERNwI39ij7TN7yHuCdfezb0kf5MuDIQgMdLjOnjKOqQqz0yCozG+X6TBzpoYa9bqLAxDGWVFVWMGvqeI+sMrNRr78rjvuBoPck0V6ccMpbU33OTVVmNur1mTgiYu5wBjIatDTU8bPVI2KQl5lZ0Rxo57j1orkhx/Y9HWxt8wWZmY1eThxDqKneQ3LNbPRz4hhCzQ3ZkFyPrDKz0aygxCHpJEkfSMuNktz/0YvuKw53kJvZaFbIY9X/EfgUcHEqqga+X8ygytX4mkoOmVjrIblmNqoVcsXxl8BpwC6AiFgHTCxmUOWspaHOVxxmNqoVkjja0/OjAkBSXXFDKm9NDTme3uw+DjMbvQpJHDdI+hYwRdJfAb8Cvl3csMpXc32OZ7fvZXd7Z6lDMTMrigGfVRURX5b0RmA7cATwmYi4ueiRlan9849vbuOIQ92iZ2ajT0EPOUyJwsmiAN1Dcp/etMuJw8xGpQETh6QdpP6NPNuAZcAnI+LJYgRWrprrn7viMDMbjQq54vgq2dze15I98PAs4MXAA8CVwOuKFVw5mpKrZuK4Kt89bmajViGd46dFxLciYkdEbI+IxcCbI+J6YGp/O0paKOkxSSskXdTL9lpJ16ft90hqydt2cSp/TNKb88o/Lmm5pD9K+qGkcQWf7TCQREtDne/lMLNRq5DE0SbpXZIq0utdQPfE2n1O8yqpErgcOAWYD5ydN4tft/OALRFxGHAp8KW073yyK5uXAQuBb0iqlDQL+AiwICKOBCpTvRGlqSHHKj92xMxGqUISx3uA9wIbgGfT8jmSxpNNDduX44EVEfFkRLQD1wGLetRZBFydlpcAJ6e5xBcB10XE3oh4imya2ONTvSpgvKQqIAesK+AchlVzfY41W3bT0dlV6lDMzIbcgIkj/eF/W0RMi4jGtLwiInZHxB397DoLWJ23viaV9VonzVG+DWjoa9+IWAt8GVgFPANsi4hf9vbhks6XtEzSstbW1oFOc0g1N+To6ArWbd0zcGUzszJTyLOqxkm6QNI3JF3Z/RqO4HqJZSrZ1chcYCZQJ+mc3upGxOKIWBARCxobG4czTJrq05Bc30FuZqNQIU1V1wCHAm8GbgdmAzsK2G8tMCdvfXYq67VOanqaDGzqZ983AE9FRGtE7AN+DLy6gFiGVXOD5+Uws9GrkMRxWER8GtgVEVcDbwVeWcB+9wHzJM2VVEPWib20R52lwLlp+Qzg1vRcrKXAWWnU1VxgHnAvWRPVCZJyqS/kZOCRAmIZVodOGkdNVYXv5TCzUamQ+zj2pfetko4E1gOHDLRTRHRIuhC4iWz005URsVzSJcCyiFgKXAFcI2kFsJk0QirVuwF4GOgALoiITuAeSUvI7iHpAB4EFhd+usOjokI01ed42iOrzGwUKiRxLE59C/9AdiUwAfh0IQePiBuBG3uUfSZveQ/wzj72/QLwhV7K/xH4x0I+v5Sa63NuqjKzUanfxCGpAtgeEVuA3wAvGpaoRoGmhhx3P7mJiCBrVTMzGx367eOIiC7g74YpllGluT5HW3snrTv3ljoUM7MhVUjn+K8k/a2kOZLqu19Fj6zMdT8l17MBmtloU0gfx5np/YK8ssDNVv3KH5K7oMV51sxGj0Imcpo7HIGMNrOn5qgQftihmY06hdw5npP0D5IWp/V5kk4tfmjlraaqghmTx/thh2Y26hTSx/FdoJ3n7tBeC3y+aBGNIs0NOVa6j8PMRplCEseLI+JfSDcCRkQb2YRONoDmhpzvHjezUaeQxNGeHqEeAJJeDHiMaQGa6uvYvKudHXv2DVzZzKxMFJI4Pgv8Apgj6QfALfjejoL4YYdmNhoVMqrql5LuB04ga6L6aERsLHpko0B34li1uY0jZ00ucTRmZkNjwMQh6T+Ba4GlEeEhQoPQfROgrzjMbDQppKnqy8CfAw9LWiLpDEnjihzXqDChtoqGuho/JdfMRpVCmqpuB26XVAm8Hvgr4EpgUpFjGxWaGvyUXDMbXQq54iCNqnoH8CHgz4CrixnUaNJc7yG5Zja6FHLn+A1ks+y9HriM7L6ODxdycEkLJT0maYWki3rZXivp+rT9HkktedsuTuWPSXpzXvmU1GT2qKRHJL2qkFhKpamhjnXbdrO3o7PUoZiZDYlCrjiuIEsWH4qI24BXS7p8oJ1S09blwCnAfOBsSfN7VDsP2BIRhwGXAl9K+84nmw3wZcBC4BvpeABfA34RES8BjmIETh2br6UhRwSs2bK71KGYmQ2JARNHRNwEvELSv0haCXwOeLSAYx8PrIiIJyOiHbgOWNSjziKea/ZaApyc5hJfBFwXEXsj4ilgBXC8pMnAa8iSGRHRHhFbC4ilZPYPyXU/h5mNEn12jks6HDg7vTYC1wOKiL8o8NizgNV562uAV/ZVJ81Rvg1oSOW/7bHvLGA30Ap8V9JRwP1k95W8YNiSpPOB8wGampoKDHnoNdVnQ3JXemSVmY0S/V1xPErWr3FqRJwUEf8GlLqhvgo4FvhmRBwD7AJe0HcCEBGLI2JBRCxobGwczhifZ9qEGnI1lR5ZZWajRn+J4+3AM8Btkr4t6WQG93DDtcCcvPXZqazXOpKqgMnApn72XQOsiYh7UvkSskQyYkmiySOrzGwU6TNxRMRPI+Is4CXAbcDHgEMkfVPSmwo49n3APElzJdWQdXYv7VFnKXBuWj4DuDUiIpWflUZdzQXmAfdGxHpgtaQj0j4nAw8XdKYl1NyQ802AZjZqFNI5visiro2It5F9838Q+FQB+3UAFwI3kY18uiEilku6RNJpqdoVQIOkFcAnSM1OEbEcuIEsKfwCuCAiupvJPgz8QNLvgaOBfyr4bEukuaGO1Vt209UVpQ7FzOygFTLn+H4RsQVYnF6F1L8RuLFH2WfylvcA7+xj3y8AX+il/CFgQeFRl15zQ472ji7Wb9/DzCnjSx2OmdlBKejOcTs4zfV+2KGZjR5OHMPguXk53M9hZuXPiWMYzJg8jqoK8bRHVpnZKODEMQyqKiuYPXW87x43s1HBiWOYNDXU8fRmN1WZWflz4hgmLWlejuw2FTOz8uXEMUya6nPs2NPB1rZ9pQ7FzOygOHEMk+75x/2wQzMrd04cw2T/49U9ssrMypwTxzBpqu++l8OJw8zKmxPHMBlXXcn0SbVOHGZW9pw4hlFzQx2rPCTXzMqcE8cwaq7P+YrDzMqeE8cwam7IsWHHXtraO0odipnZAXPiGEZNaUiuR1aZWTkrauKQtFDSY5JWSHrB3OBphr/r0/Z7JLXkbbs4lT8m6c099quU9KCknxcz/qHW7JFVZjYKFC1xSKoELgdOAeYDZ0ua36PaecCWiDgMuBT4Utp3PtlUsy8DFgLfSMfr9lGyWQXLyv57OZw4zKyMFfOK43hgRUQ8GRHtwHXAoh51FgFXp+UlwMmSlMqvi4i9EfEUsCIdD0mzgbcC3yli7EUxJVfDpHFVftihmZW1YiaOWcDqvPU1qazXOmmO8m1AwwD7fhX4O6Crvw+XdL6kZZKWtba2Hug5DLmWaXVuqjKzslZWneOSTgU2RMT9A9WNiMURsSAiFjQ2Ng5DdIVp8pBcMytzxUwca4E5eeuzU1mvdSRVAZOBTf3seyJwmqSVZE1fr5f0/WIEXyzNDTnWbt3Nvs5+L5jMzEasYiaO+4B5kuZKqiHr7F7ao85S4Ny0fAZwa2QTViwFzkqjruYC84B7I+LiiJgdES3peLdGxDlFPIch11xfR2dXsG7r7lKHYmZ2QKqKdeCI6JB0IXATUAlcGRHLJV0CLIuIpcAVwDWSVgCbyZIBqd4NwMNAB3BBRHQWK9bh1NTw3JDc7ketm5mVE42FGekWLFgQy5YtK3UYALTu2MsJ/3wLLzl0Il9+51G8dMakUodkZtYrSfdHxIKe5WXVOT4aNE6s5fJ3H8v6bXs47bI7+Oqv/kR7h/s7zKx8OHGUwMIjD+XmT7yWU46cwVd/9TinXXYHf1y7rdRhmZkVxImjROrravj62cew+L3HsXlXO4suv5P/d9Oj7O0YFV05ZjaKOXGU2Jtedig3f/y1nH70LC6/7QlO/fodPLhqS6nDMjPrkxPHCDA5V81X3nUU3/3An7Fzbwfv+OZd/NONj7Bnn68+zGzkceIYQf7iiEO46eOv4cw/m8Pi3zzJW772PyxbubnUYZmZPY8TxwgzaVw1//z2V/D9815Je2cX7/zW3Xx26XJP/mRmI4YTxwh10rxp3PSx1/DeE5q56q6VLPzq/3DXExtLHZaZmRPHSFZXW8Uli47k+vNPQIJ3f/se/v4nf2DnXl99mFnpOHGUgVe+qIFffPQ1nHfSXK69dxVvvvQ3/OZPI+dR8WY2tjhxlInxNZV8+tT5LPnQqxlXXcH7rryXv1vyO7bt3lfq0MxsjHHiKDPHNU/lvz7y53zotS9myf1reNOlt3PLI8+WOiwzG0OcOMrQuOpKLjrlJfzkb05kyvgazrt6GR+//iG2trWXOjQzGwOK9lh1K76j5kxh6YdP5PJbV/CNXz/B7X9q5e3HzGLR0bM4ctYksunbzaP92jQAAA+gSURBVMyGlh+rPkosX7eNS29+nNv/tIF9ncGLGutYdNQsFh09k5ZpnvfDzAavr8eqFzVxSFoIfI1sIqfvRMQXe2yvBb4HHEc2ZeyZEbEybbsYOA/oBD4SETdJmpPqTwcCWBwRXxsojrGQOLptbWvnv/+4np8+uJZ7nsruOj9qzhQWHTWTU4+awSETx5U4QjMrF8OeOCRVAn8C3gisIZtK9uyIeDivzt8Ar4iID0k6C/jLiDhT0nzgh8DxwEzgV8DhwCHAjIh4QNJE4H7g9Pxj9mYsJY5867bu5ue/X8dPH1zHw89sp0Jw4mHTOO2omSw88lAmjqsudYhmNoKVInG8CvhsRLw5rV8MEBH/nFfnplTnbklVwHqgEbgov25+vR6f8TPgsoi4ub9YxmriyPf4sztY+rt1/Oyhdaza3EZNVQVveOkhLDp6Fq87opHaqspSh2hmI0xfiaOYneOzgNV562uAV/ZVJ81Rvg1oSOW/7bHvrPwdJbUAxwD3DGXQo9W86RP55JuO4BNvPJwHV2/lZw+u5ee/f4Yb/7CeieOqeMuRM1h0zExeObeBygp3qptZ38pyVJWkCcB/AB+LiO191DkfOB+gqalpGKMb2SRxbNNUjm2ayqdPnc+dT2ziZw+t5ee/X8f1y1YzfVItb3vFTE4/ZhYvm+mRWWb2QsVMHGuBOXnrs1NZb3XWpKaqyWSd5H3uK6maLGn8ICJ+3NeHR8RiYDFkTVUHdSajVFVlBa89vJHXHt7I7tM7ueXRZ/npg+u4+u6VfOeOp3hRYx2nHz2L04+eRVNDrtThmtkIUcw+jiqyzvGTyf7o3we8OyKW59W5AHh5Xuf42yPiXZJeBlzLc53jtwDzgC7gamBzRHys0FjcxzE4PUdmSfCGl07ngyfO5YQX1fsqxGyMGPY+jtRncSFwE9lw3CsjYrmkS4BlEbEUuAK4RtIKYDNwVtp3uaQbgIeBDuCCiOiUdBLwXuAPkh5KH/V/IuLGYp3HWDQlV8PZxzdx9vFNrNu6m+vuXcX371nFzQ8/y0tnTOKDJ7Zw2tEz3aFuNkb5BkAryJ59nfzsobVcecdKHnt2B9Mm1HDOCc2cc0Iz0ybUljo8MyuCktwAOFI4cQydiODOFZu48s6nuPXRDdRUVrDo6Jl84MS5zJ85qdThmdkQKsVwXBuFJHHSvGmcNG8aT7Tu5Ko7V7Lk/jX86P41vOpFDZx30lxe/5JDqPCQXrNRy1ccdtC2te3juvtWcfVdK1m3bQ8tDTk+cOJczjhuNnW1/m5iVq7cVOXEUXT7Oru4afl6rrzjKR5YtZWJ46o468/mcO6rW5g91cN5zcqNE4cTx7B6cNUWrrxzJTf+4RkigoVHHsoHT5zLcc1TPZzXrEw4cThxlMS6rbv53t1P88N7V7Ft9z6Omj2ZD540l7e8fAbVlZ5HzGwkc+Jw4iiptvYOfvzAWq688ymebN3F9Em1/Pm8RqZPquWQieOy90njOGRiLY0Ta0fUPSLtHV3s3tfJpHFVvlqyMcWjqqykcjVVnHNCM+8+vonbH2/le3et5I7HN9K6cy+dXS/88jI1V830SeNonFjL9JRQpk/KEkzjxO73wSWYjs4utu3ex9bd+9ja1s7Wtn1sactfbn/etq1p2672TgBqqiqYPqmWQyeNS7GMy5Ynj2P6xFoOnZyVjaseOUnPrBh8xWEl1dUVbNrVzoYde9iwfS8bduzh2e17eXb7Hjbs2MuG7vcdfSeYQyaO45B05TJtYg172jvZuvuFSWHHno4+46hQdsf8lFw1U8ZXMzVXw+Rc9j41V01tVSWtO7O41m/bk71v38OefV0vONbk8dW9JpT9CWdyLdPqaj1k2UY8X3HYiFRRIRpT89TLZvZdr6sr2NzWniWUvATz3PteVmzYyMade8nVVGUJIFfD1FwNL5pW9/ykUFeTrecliIm1VYP+Qx4RbN/TwbPb9zwvoTy7fS/rU9lj67fTumMvPXNeVTrvQyaNo3FC7f6fQeOEmrzlLBHmavzP1EYW/0ZaWaioENMm1DJtQv8JZjhJYvL4aiaPr+bw6RP7rNfR2cWmXe2s35ZdpWxIVyvrt2WJb82WNh5avYVNu9rprQGgrqbyuWQysZbG9HN4XtnEWhrqaqmp8oADKz4nDrMiq6qs2N8nclQ/9To6u9jc1k7rjr37Xxt3pvWde2ndsYfH1u/gzp2b2LZ7X6/HmJKrpnFCbXZVla6opuSqmZyrZsr4rNmtezm7KqtmfHWlO/1tUJw4zEaIqsqKrL9m4rgB6+7Z18mmXT2TTPa+YccetrTt4+lNbfxuzVa2tO2jveOFfTHdaqoqmDK+OjXldfftZE19k/PLx1fTFcG+zi72dXa/d/Wy/vzl9o4uOrq62NeRlbd3dtHR+dwyQENdDdMm1NIwoZZpE7qXn3sfSaPszInDrCyNq65k1pTxzJoyvqD6e/Z1ZqPG0mixbbvT8u5s4MC27lFku9tZvbmNP67Nynvr/C9UVYWorqygqlLUVFZQXVlBdVVWVl3x3HJXwMpNu9i0s522NIKtp4njqlJTZQ0NdbVMm5jeJzw/4TRMqC3psOmIoLMr6EzvHV1BV3rvzHt1PG+5i64ukKCyQlRWiApl75USFRXkLee9529P9YfrvJ04zMaAcdWVzJg8nhmTC0s03fbs68yGMLftY9vufVRWQFVFlgRqqroTQwXVecmhqlJUV1Qc0KixtvYONu1sp3XnXjbtbGfTzuxKauPOdjamsidad3Lvyna2tPXeJ1RTWUHDhBrqaqvoHjUa+/+TveWXx/7yVBY877g963b1lRRSeSlJPC/BVFaIZf/whiEfIu7EYWZ9GlddybjqSqZPGrj5bCjkaqrI1Vcxp37gZ5t19wltyksq+Ummrb0DIUj5S7D/G3m23Ht594LSmp47xPOuCrI/zFmirJCo6i5Pr6oeyxX7yyqorCDbN+8KoyuyRNSdgLIERS9lPba/oCyr39WVba8qwrDvoiYOSQuBr5HNAPidiPhij+21wPeA48jmGj8zIlambRcD5wGdwEci4qZCjmlmY8Ng+oRsaBVt7J6kSuBy4BRgPnC2pPk9qp0HbImIw4BLgS+lfeeTTSP7MmAh8A1JlQUe08zMiqiYg76PB1ZExJMR0Q5cByzqUWcRcHVaXgKcrOyacRFwXUTsjYingBXpeIUc08zMiqiYiWMWsDpvfU0q67VORHQA24CGfvYt5JgASDpf0jJJy1pbWw/iNMzMLN+ovc00IhZHxIKIWNDY2FjqcMzMRo1iJo61wJy89dmprNc6kqqAyWSd5H3tW8gxzcysiIqZOO4D5kmaK6mGrLN7aY86S4Fz0/IZwK2RDZpeCpwlqVbSXGAecG+BxzQzsyIq2nDciOiQdCFwE9nQ2SsjYrmkS4BlEbEUuAK4RtIKYDNZIiDVuwF4GOgALoiIToDejlmsczAzsxfyfBxmZtarMT11rKRW4OkD3H0asHEIwymmcooVyivecooVyivecooVyiveg421OSJeMLpoTCSOgyFpWW8ZdyQqp1ihvOItp1ihvOItp1ihvOItVqyjdjiumZkVhxOHmZkNihPHwBaXOoBBKKdYobziLadYobziLadYobziLUqs7uMwM7NB8RWHmZkNihOHmZkNihNHHyQtlPSYpBWSLip1PP2RNEfSbZIelrRc0kdLHdNA0vwqD0r6ealjGYikKZKWSHpU0iOSXlXqmPoi6ePpd+CPkn4oaUTNciTpSkkbJP0xr6xe0s2SHk/vU0sZY74+4v1/6Xfh95J+ImlKKWPs1luseds+KSkkTRuKz3Li6EUZThjVAXwyIuYDJwAXjPB4AT4KPFLqIAr0NeAXEfES4ChGaNySZgEfARZExJFkj+U5q7RRvcBVZJOz5bsIuCUi5gG3pPWR4ipeGO/NwJER8QrgT8DFwx1UH67ihbEiaQ7wJmDVUH2QE0fvymrCqIh4JiIeSMs7yP6w9TpPyUggaTbwVuA7pY5lIJImA68he64aEdEeEVtLG1W/qoDx6WnTOWBdieN5noj4Ddlz6fLlT+h2NXD6sAbVj97ijYhfpvmDAH5L9pTukuvjZwvZ7Kp/BwzZSCgnjt4VPGHUSCOpBTgGuKe0kfTrq2S/yF2lDqQAc4FW4Lupae07kupKHVRvImIt8GWyb5bPANsi4peljaog0yPimbS8HpheymAG6YPAf5c6iL5IWgSsjYjfDeVxnThGEUkTgP8APhYR20sdT28knQpsiIj7Sx1LgaqAY4FvRsQxwC5GVlPKfqlvYBFZspsJ1Ek6p7RRDU6aVqEs7hGQ9PdkzcQ/KHUsvZGUA/4P8JmhPrYTR+/KbsIoSdVkSeMHEfHjUsfTjxOB0yStJGsCfL2k75c2pH6tAdZERPcV3BKyRDISvQF4KiJaI2If8GPg1SWOqRDPSpoBkN43lDieAUl6P3Aq8J4YuTfDvZjsS8Tv0r+32cADkg492AM7cfSurCaMkiSyNvhHIuJfSx1PfyLi4oiYHREtZD/XWyNixH4rjoj1wGpJR6Sik8nmiRmJVgEnSMql34mTGaEd+T3kT+h2LvCzEsYyIEkLyZpaT4uItlLH05eI+ENEHBIRLenf2xrg2PQ7fVCcOHqROr66J4x6BLhhhE8YdSLwXrJv7w+l11tKHdQo8mHgB5J+DxwN/FOJ4+lVuipaAjwA/IHs3/eIejyGpB8CdwNHSFoj6Tzgi8AbJT1OdtX0xVLGmK+PeC8DJgI3p39r/17SIJM+Yi3OZ43cqywzMxuJfMVhZmaD4sRhZmaD4sRhZmaD4sRhZmaD4sRhZmaD4sRhNgQkdeYNhX5oKJ+oLKmltyeempVKVakDMBsldkfE0aUOwmw4+IrDrIgkrZT0L5L+IOleSYel8hZJt6Y5HW6R1JTKp6c5Hn6XXt2PDKmU9O0018YvJY0v2UnZmOfEYTY0xvdoqjozb9u2iHg52R3HX01l/wZcneZ0+AHw9VT+deD2iDiK7JlY3U8smAdcHhEvA7YC7yjy+Zj1yXeOmw0BSTsjYkIv5SuB10fEk+lBlOsjokHSRmBGROxL5c9ExDRJrcDsiNibd4wW4OY00RGSPgVUR8Tni39mZi/kKw6z4os+lgdjb95yJ+6ftBJy4jArvjPz3u9Oy3fx3LSu7wH+Jy3fAvw17J+XffJwBWlWKH9rMRsa4yU9lLf+i4joHpI7NT1Zdy9wdir7MNmsgv+bbIbBD6TyjwKL05NNO8mSyDOYjSDu4zArotTHsSAiNpY6FrOh4qYqMzMbFF9xmJnZoPiKw8zMBsWJw8zMBsWJw8zMBsWJw8zMBsWJw8zMBuX/A2AsxzEbacugAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Plot:\n",
        "plt.plot(range(0, len(loss4)), loss4)\n",
        "plt.title(\"Average Loss vs. Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0rrCHF0k32r"
      },
      "source": [
        "### (c) (5 points) Report the final testing accuracy of trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OQpqDtOHWxpp"
      },
      "outputs": [],
      "source": [
        "## Code:\n",
        "\n",
        "\n",
        "def Test_Accuracy(model, test_data_loader):\n",
        "\n",
        "  iters = 0\n",
        "  correct = 0\n",
        "\n",
        "  for images, labels in test_data_loader:\n",
        "\n",
        "    images = images.view(-1, 28*28)\n",
        "    labels = labels.float()\n",
        "    y_predict = model(images)\n",
        "    # Converting back to the 0, 1 labels\n",
        "    y_predict_coded = y_predict.data.sign() / 2 + 0.5\n",
        "    iters += labels.size(0)\n",
        "    correct += (labels == y_predict_coded.view(-1).long()).sum().item()\n",
        "\n",
        "  accuracy = correct / iters\n",
        "  print(\"Accuracy is: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_4vWbCsaMMt",
        "outputId": "6c284ca1-c3a0-479d-e167-a8af470beb69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy is:  0.9995271867612293\n"
          ]
        }
      ],
      "source": [
        "Test_Accuracy(model4, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzs9iWPxXTRJ"
      },
      "source": [
        "### 2. (20 points) Implement **Linear SVM** with Pytorch to do handwritten digit 0 vs. 1 classification. Pick an optimizer yourself. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "L1J06Z-dXiEy"
      },
      "outputs": [],
      "source": [
        "## Implementation of Linear SVM\n",
        "## You can insert more code chunks and text cells if you want to.\n",
        "## Your code:\n",
        "\n",
        "# The number of epochs is at least 10, you can increase it to achieve better performance\n",
        "num_epochs = 10\n",
        "\n",
        "# Here is my special loss class:\n",
        "class SVM_hinge(torch.nn.Module):\n",
        "\n",
        " def __init__(self):\n",
        "   super(SVM_hinge,self).__init__()\n",
        "\n",
        " def forward(self, labels, y_predict):\n",
        "   # Instead of feeding through an activation function, we'll just take the sign of the output\n",
        "   y_predict = torch.mean(torch.clamp(1 - labels.t() * y_predict, min=0))\n",
        "   return y_predict\n",
        "\n",
        "\n",
        "# Training the Model\n",
        "\n",
        "def hype_tune_SVM(num_epochs, lr, batch_size, momentum):\n",
        "  print(\"\")\n",
        "  print(\" >>>> Number of Epochs: \", num_epochs, \" Learning Rate: \", lr, \" Batch Size: \", batch_size, \"<<<<\")\n",
        "  print(\"\")\n",
        "\n",
        "  ### Intantiating the model\n",
        "  model = torch.nn.Linear(28*28, 1)\n",
        "\n",
        "  ### I am going to use my hinge loss class\n",
        "  criterion = SVM_hinge()\n",
        "  iter = 0\n",
        "  losses = list()\n",
        "  optimizer = optim.SGD(model.parameters(),lr=lr, momentum = momentum)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      epoch_losses = list()\n",
        "\n",
        "      for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "          # Convert the 28*28 image matrix into a 784-dim vector\n",
        "          images = images.view(-1, 28*28) \n",
        "          # Convert labels from 0,1 to -1,1 is necessary with Hinge Loss\n",
        "          labels = 2*(labels.float()-0.5)\n",
        "          labels = labels.float()\n",
        "          \n",
        "          ## TODO \n",
        "\n",
        "          # 1. Compute Loss. Check torch functions for the corresponding loss for Logistic and SVM\n",
        "          optimizer.zero_grad()\n",
        "          y_predict = model(images)\n",
        "          # labels = labels.unsqueeze(1) \n",
        "          loss = criterion(y_predict, labels)\n",
        "\n",
        "          # 2. Do optimization. Chec torch.optim to see how to do optimization with pytorch\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          iter += 1\n",
        "\n",
        "          # 3. Save batch loss\n",
        "          epoch_losses.append(loss.item()) \n",
        "\n",
        "      ## Save average epoch loss\n",
        "      losses.append(np.average(epoch_losses))\n",
        "      print('Epoch: {} :: Iteration: {} :: Loss: {}  '.format(epoch, iter, losses[-1], ))\n",
        "\n",
        "  return losses, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEzo4nBuwmmB",
        "outputId": "15eace08-5e6e-498f-aa87-20460f86f4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.01  Batch Size:  64 <<<<\n",
            "\n",
            "Epoch: 0 :: Iteration: 198 :: Loss: 0.013286388742577548  \n",
            "Epoch: 1 :: Iteration: 396 :: Loss: 0.00382909941195388  \n",
            "Epoch: 2 :: Iteration: 594 :: Loss: 0.002763893370601264  \n",
            "Epoch: 3 :: Iteration: 792 :: Loss: 0.002113555586247733  \n",
            "Epoch: 4 :: Iteration: 990 :: Loss: 0.001776341979175505  \n",
            "Epoch: 5 :: Iteration: 1188 :: Loss: 0.0013254885185472291  \n",
            "Epoch: 6 :: Iteration: 1386 :: Loss: 0.0012982834995997072  \n",
            "Epoch: 7 :: Iteration: 1584 :: Loss: 0.0011069351863680463  \n",
            "Epoch: 8 :: Iteration: 1782 :: Loss: 0.0009674362255015759  \n",
            "Epoch: 9 :: Iteration: 1980 :: Loss: 0.0008733005714461659  \n",
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  64 <<<<\n",
            "\n",
            "Epoch: 0 :: Iteration: 198 :: Loss: 0.033394253361179974  \n",
            "Epoch: 1 :: Iteration: 396 :: Loss: 0.012167067596256131  \n",
            "Epoch: 2 :: Iteration: 594 :: Loss: 0.007010018862219471  \n",
            "Epoch: 3 :: Iteration: 792 :: Loss: 0.004341039453830683  \n",
            "Epoch: 4 :: Iteration: 990 :: Loss: 0.003381744229394679  \n",
            "Epoch: 5 :: Iteration: 1188 :: Loss: 0.002587705690677118  \n",
            "Epoch: 6 :: Iteration: 1386 :: Loss: 0.0013184220849940874  \n",
            "Epoch: 7 :: Iteration: 1584 :: Loss: 0.0006974054867345276  \n",
            "Epoch: 8 :: Iteration: 1782 :: Loss: 0.000696459004298003  \n",
            "Epoch: 9 :: Iteration: 1980 :: Loss: 0.00024868912919603215  \n",
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.01  Batch Size:  64 <<<<\n",
            "\n",
            "Epoch: 0 :: Iteration: 198 :: Loss: 0.021460911931677  \n",
            "Epoch: 1 :: Iteration: 396 :: Loss: 0.006676371468024122  \n",
            "Epoch: 2 :: Iteration: 594 :: Loss: 0.0042777932060863635  \n",
            "Epoch: 3 :: Iteration: 792 :: Loss: 0.003236161239181805  \n",
            "Epoch: 4 :: Iteration: 990 :: Loss: 0.002517287746410478  \n",
            "Epoch: 5 :: Iteration: 1188 :: Loss: 0.0019033503944449353  \n",
            "Epoch: 6 :: Iteration: 1386 :: Loss: 0.0016244424629316787  \n",
            "Epoch: 7 :: Iteration: 1584 :: Loss: 0.0012145223509934214  \n",
            "Epoch: 8 :: Iteration: 1782 :: Loss: 0.001089037565345114  \n",
            "Epoch: 9 :: Iteration: 1980 :: Loss: 0.0010405025031003687  \n",
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  64 <<<<\n",
            "\n",
            "Epoch: 0 :: Iteration: 198 :: Loss: 0.0735536289890532  \n",
            "Epoch: 1 :: Iteration: 396 :: Loss: 0.02795943713775187  \n",
            "Epoch: 2 :: Iteration: 594 :: Loss: 0.015799122241636116  \n",
            "Epoch: 3 :: Iteration: 792 :: Loss: 0.011629917103834826  \n",
            "Epoch: 4 :: Iteration: 990 :: Loss: 0.007214791941070798  \n",
            "Epoch: 5 :: Iteration: 1188 :: Loss: 0.004445244228900081  \n",
            "Epoch: 6 :: Iteration: 1386 :: Loss: 0.003318277516900891  \n",
            "Epoch: 7 :: Iteration: 1584 :: Loss: 0.00047599548718543967  \n",
            "Epoch: 8 :: Iteration: 1782 :: Loss: 0.0008149805074237814  \n",
            "Epoch: 9 :: Iteration: 1980 :: Loss: 0.0002250429525068312  \n"
          ]
        }
      ],
      "source": [
        "loss1_SVM, model1_SVM = hype_tune_SVM(10, 0.01, 64, 0.75)\n",
        "loss2_SVM, model2_SVM = hype_tune_SVM(10, 0.1, 64, 0.75)\n",
        "loss3_SVM, model3_SVM = hype_tune_SVM(10, 0.01, 64, 0.9)\n",
        "loss4_SVM, model4_SVM = hype_tune_SVM(10, 0.1, 64, 0.9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk4lTWisvDBb"
      },
      "source": [
        "I used the following resources in constructing this code: \n",
        "\n",
        "https://bytepawn.com/svm-with-pytorch.html\n",
        "\n",
        "https://torchmetrics.readthedocs.io/en/stable/classification/hinge_loss.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWwSIMRRZtMX"
      },
      "source": [
        "### (a) (5 points) Report the hyper-parameters (number of epochs, learning rate, momentum etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICpUuHB_GKnx",
        "outputId": "336f73c4-7c61-4a09-d04d-9dba3dd286c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0002250429525068312\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "list_of_losses_SVM = [loss1_SVM[-1], loss2_SVM[-1], loss3_SVM[-1], loss4_SVM[-1]]\n",
        "min_loss_SVM = min(list_of_losses_SVM)\n",
        "print(min_loss_SVM)\n",
        "min_params_SVM = list_of_losses_SVM.index(min_loss_SVM)\n",
        "print(min_params_SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-6Cx2r9Ztnj"
      },
      "source": [
        "Answer: The lowest loss of those hyper-parameter combinations I tried occurred when number of epochs = 10, learning rate = 0.1, batch size = 64, and momentum  = 0.9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBBIhPzMXctJ"
      },
      "source": [
        "### (b) (10 points) Report the **Average loss of an epoch** for every epoch by generating Average Loss vs. Epoch plot. Please report at least **10** epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "JH6w-fvXXkLe",
        "outputId": "c9d67a71-9e0c-4fc9-bbce-3f56e5bc6bfe"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Plot:\n",
        "\n",
        "## Plot:\n",
        "plt.plot(range(0, len(loss4_SVM)), loss4_SVM)\n",
        "plt.title(\"Average Loss vs. Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-iSjv9UX-z2"
      },
      "source": [
        "### (c) (5 points) Report the final testing accuracy of trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvIfrVU7YBMH",
        "outputId": "027517ad-78a9-4994-e41c-5a9b1a1c7e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy is:  0.9990543735224586\n"
          ]
        }
      ],
      "source": [
        "## Code:\n",
        "\n",
        "Test_Accuracy(model4_SVM, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kegz8m6glB3d"
      },
      "source": [
        "## Problem 2 (60 points)\n",
        "\n",
        "In this problem you will practice implementing MLP and CNN to classify daily life images (CIFAR10).\n",
        "\n",
        "**Data.** You will use CIFAR10 classification dataset (10 classes). Pytorch/torchvision has provide a useful dataloader to automatically download and load the data into batches. Code of the data loader has been provided in the template. Please don't modify the data loading part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJStVuRpqkzS",
        "outputId": "02ea4b7d-1a64-4b24-ba41-4024ff01ad37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "50000\n"
          ]
        }
      ],
      "source": [
        "## Data loading code chunk, please don't modify it. \n",
        "## However, you can adjust the batch size if you want to.\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.utils.data as td\n",
        "import random, time\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "batch_size_cifar = 100\n",
        "data_dir = './data'\n",
        "\n",
        "def cifar_loaders(batch_size, shuffle_test=False): \n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.225, 0.225, 0.225])\n",
        "    train = datasets.CIFAR10(data_dir, train=True, download=True, \n",
        "        transform=transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]))\n",
        "    # Once you have downloaded the data by setting download=True, you can\n",
        "    # change download=True to download=False\n",
        "    test = datasets.CIFAR10(data_dir, train=False, \n",
        "        transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
        "        shuffle=True, pin_memory=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
        "        shuffle=shuffle_test, pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader_cifar, test_loader_cifar = cifar_loaders(batch_size_cifar)\n",
        "print(len(train_loader_cifar.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZWcD3ufnEcJ"
      },
      "source": [
        "### **Problem Description.**\n",
        "### 1. (20 points) Implement a 7 layers fully-connected neural networks with ReLU activation to do image classification. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5VxD0V8Qg4s",
        "outputId": "16a0581d-e1cb-473d-f3f9-6bea747c235f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CWHfa3bnnIpD"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      super(NeuralNet, self).__init__()\n",
        "      # With a neural net we need to flatten\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.feature = nn.Sequential(nn.Linear(3072,180), nn.ReLU(True), \n",
        "                                   nn.Linear(180,360), nn.ReLU(True),\n",
        "                                   nn.Linear(360,200), nn.ReLU(True),\n",
        "                                   nn.Linear(200,250), nn.ReLU(True),\n",
        "                                   nn.Linear(250,80), nn.ReLU(True),\n",
        "                                   nn.Linear(80,120), nn.ReLU(True),\n",
        "                                   nn.Linear(120,10), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.feature(x)\n",
        "      output = x\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FauxKKfsLBcS"
      },
      "outputs": [],
      "source": [
        "def hype_tune_NN(num_epochs, lr, batch_size, train_loader_cifar = train_loader_cifar):\n",
        "  print(\"\")\n",
        "  print(\" >>>> Number of Epochs: \", num_epochs, \" Learning Rate: \", lr, \" Batch Size: \", batch_size, \"<<<<\")\n",
        "  print(\"\")\n",
        "\n",
        "  ### Intantiating the model\n",
        "  model_NN = NeuralNet()\n",
        "  model_NN.to(device)\n",
        "\n",
        "  ### I am going to use the Binary Cross Entropy Loss for Logistic\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  iter = 0\n",
        "  losses = list()\n",
        "  optimizer = optim.SGD(model_NN.parameters(),lr=lr)\n",
        "  print(\"Starting for loop\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      epoch_losses = list()\n",
        "\n",
        "      for i, (images, labels) in enumerate(train_loader_cifar):\n",
        "\n",
        "          ## TODO \n",
        "\n",
        "          # 1. Compute Loss. Check torch functions for the corresponding loss for Logistic and SVM\n",
        "          # print(i, images.size())\n",
        "          images = images.view(100, -1)\n",
        "\n",
        "          # print(images.size())\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          y_predict = model_NN(images)\n",
        "          loss = criterion(y_predict, labels)\n",
        "          \n",
        "          # 2. Do optimization. Chec torch.optim to see how to do optimization with pytorch\n",
        " \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          iter += 1\n",
        "\n",
        "          # 3. Save batch loss\n",
        "          epoch_losses.append(loss.item()) \n",
        "\n",
        "      ## Save average epoch loss\n",
        "        # if (iter) % 64 == 0:\n",
        "      losses.append(np.average(epoch_losses))\n",
        "      print('Epoch: {} :: Iteration: {} :: Loss: {}  '.format(epoch, iter, losses[-1], ))\n",
        "\n",
        "  return losses, model_NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQzxnOj8LNtI",
        "outputId": "de64f5c2-adba-4425-a72a-5548a6c40366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  100 <<<<\n",
            "\n",
            "Starting for loop\n",
            "Epoch: 0 :: Iteration: 500 :: Loss: 2.2751150403022766  \n",
            "Epoch: 1 :: Iteration: 1000 :: Loss: 2.1113530759811403  \n",
            "Epoch: 2 :: Iteration: 1500 :: Loss: 1.910115234375  \n",
            "Epoch: 3 :: Iteration: 2000 :: Loss: 1.8131835632324218  \n",
            "Epoch: 4 :: Iteration: 2500 :: Loss: 1.7564674370288849  \n",
            "Epoch: 5 :: Iteration: 3000 :: Loss: 1.7099139459133148  \n",
            "Epoch: 6 :: Iteration: 3500 :: Loss: 1.5621167159080505  \n",
            "Epoch: 7 :: Iteration: 4000 :: Loss: 1.5201233100891114  \n",
            "Epoch: 8 :: Iteration: 4500 :: Loss: 1.4909598872661591  \n",
            "Epoch: 9 :: Iteration: 5000 :: Loss: 1.468035308599472  \n",
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.01  Batch Size:  100 <<<<\n",
            "\n",
            "Starting for loop\n",
            "Epoch: 0 :: Iteration: 500 :: Loss: 2.3026757760047913  \n",
            "Epoch: 1 :: Iteration: 1000 :: Loss: 2.3019942064285277  \n",
            "Epoch: 2 :: Iteration: 1500 :: Loss: 2.3012231154441833  \n",
            "Epoch: 3 :: Iteration: 2000 :: Loss: 2.2999197540283203  \n",
            "Epoch: 4 :: Iteration: 2500 :: Loss: 2.2969357061386106  \n",
            "Epoch: 5 :: Iteration: 3000 :: Loss: 2.289559669017792  \n",
            "Epoch: 6 :: Iteration: 3500 :: Loss: 2.2709181337356568  \n",
            "Epoch: 7 :: Iteration: 4000 :: Loss: 2.2197330741882326  \n",
            "Epoch: 8 :: Iteration: 4500 :: Loss: 2.1918117928504945  \n",
            "Epoch: 9 :: Iteration: 5000 :: Loss: 2.180338957309723  \n"
          ]
        }
      ],
      "source": [
        "NN_losses, NN_mod = hype_tune_NN(10, 0.1, 100)\n",
        "NN_losses2, NN_mod2 = hype_tune_NN(10, 0.01, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUJzcn1QGwPG"
      },
      "source": [
        "Of the two sets of hyperparameters I tried with the fully-connected neural net, the one with 10 epochs, a learning rate of 0.1, and a batch size of 100 with no momentum had the lower loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtW8DhiBN01f"
      },
      "source": [
        "Note: I used the following resources to create the NN Code:\n",
        "\n",
        "https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7AJxlCVZA4L"
      },
      "source": [
        "### (a) (5 points) Print the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK8zxDXqZSLw",
        "outputId": "299475d0-c8cf-4706-b5a0-9ef1d18bd880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNet(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (feature): Sequential(\n",
            "    (0): Linear(in_features=3072, out_features=180, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=180, out_features=360, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=360, out_features=200, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=200, out_features=250, bias=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Linear(in_features=250, out_features=80, bias=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Linear(in_features=80, out_features=120, bias=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Linear(in_features=120, out_features=10, bias=True)\n",
            "    (13): ReLU(inplace=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "## Print the model:\n",
        "print(NeuralNet())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYWHWEGinOyC"
      },
      "source": [
        "### (b) (10 points) Report the **Average loss of an epoch** for every epoch by generating Average Loss vs. Epoch plot. Please report at least **10** epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "L8pJfqcHm89-",
        "outputId": "8d04b736-db80-442e-da74-4c0a37ea60e3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gU19n+8e+jApJQo0gCgei928jA6957TWxw7+HnJI7txE4c+03ixOl5EydOcWxi3HtN3HvBcQGETQfbgOlNdCSayvP7YwYsyxIIpNVI2vtzXXOxOzOafbTA3jvnzJxj7o6IiMSvhKgLEBGRaCkIRETinIJARCTOKQhEROKcgkBEJM4pCERE4pyCQEQahJkdaWbLoq5D9p2CQOrFzN4xsw1m1jrqWurLzH5uZg9FXUdDMLPuZuZmVlJtGRt1bdL0JEVdgDRfZtYdOAzYBJwOPNnAx09y9/KGPGYcytZ7KHujMwKpj4uBj4D7gEsAzKy1mW00s8G7djKzHDPbZma54fNTzWxauN8HZja0yr6LzOxGM5sBlJpZkpn92MwWmNkWM5tjZmdV2T/RzP5kZmvN7Aszuzr8JpwUbs8yswlmttLMlpvZr8wscV9/UTM73cxmhzW/Y2YDqmy7MTz2FjP71MyOCdePNLMiM9tsZqvN7LZajj3XzE6t8jzJzIrN7EAzSzGzh8xsXfjaU8wsb1/rr+E17zOzO83s9bDud82sW5XtB4evtSn88+Aq29qZ2b1mtiI8G/x3tWNfb2Zrwvf8svrWKo3A3bVo2a8FmA98BxgBlAF54fp7gF9X2e+7wCvh4wOANcAoIJEgQBYBrcPti4BpQAGQGq47B8gn+OIyFigFOoXbrgLmAF2AtsAbgANJ4fZngbuANkAuMBn4f7X8Pj8HHqphfd/wNY8DkoEfhb97K6AfsBTID/ftDvQKH38IXBQ+TgdG1/K6PwMervL8FGBu+Pj/Ac8DaeH7NQLIrMPfTfeq70MN2+8DtgCHA62B24H/htvaARuAiwhaDc4Ln7cPt78IPB6+38nAEeH6I4Fy4NZw/cnAVqBt1P9Wtezl30vUBWhpngtwaPjh3yF8Pg/4fvj4WGBBlX3fBy4OH/8T+GW1Y31a5cNkEXD5Xl57GnBG+Pitqh/s4Wt7+AGWB+wgDJRw+3nA27Uct7Yg+CnwRJXnCcDy8IOvN0GwHQskV/u5icAvdr1He/h9eocfymnh84eBn4WPLwc+AIbu49/PriDYWG0ZEG6/D3isyv7pQAVBAF8ETK52vA+BS4FOQGVNH+7h+7GNKuETvjc1BqCWprOoaUj21yXAa+6+Nnz+SLgO4G0gzcxGhf0Iwwm+mQN0A64Pmzk2mtlGgg+f/CrHXlr1hczs4ipNSRuBwUCHcHN+tf2rPu5G8M10ZZWfvYvgzGBf5AOLdz1x98rwdTq7+3zgOoIQWWNmj5nZrt/lCoKziXlh88qp1CA8xlzgNDNLI+hveSTc/CDwKvBY2BTzBzNL3ofaO7h7dpVlbpVtu98rdy8B1oe/61d+39BioDPB39V6d99Qy+ut86/2SWwlCBlpwtRZLPvMzFKBMUCima0KV7cGss1smLtPN7MnCL59rwZecPct4X5LCZqNfr2Hl9g9JG7Ybv0v4BjgQ3evMLNpgIW7rCRoFtqloMrjpQRnBB28fh2mK4AhVWqy8HWWA7j7I8AjZpZJEDS/J2gS+hw4z8wSgG8AT5lZe3cvreE1HiV4vxKAOWE44O5lBGcVvwhD9SWCM6gJ9fh9dtn9XplZOkGT0Ipw6VZt367AKwTvaTszy3b3jQ1QgzQBOiOQ/XEmQTPCQIJv+8OBAcB7BB3IEHyjHQtcwJffbiH4UL8qPFswM2tjZqeYWUYtr9WGIBiKAcLOx8FVtj8BXGtmnc0sG7hx1wZ3Xwm8BvzJzDLNLMHMepnZEXv43RLCDtpdS+vwNU4xs2PCb+PXEwTMB2bWz8yODvfbTtA0UhnWeqGZ5YRnELs+NCtred3HgOOBb1d9v8zsKDMbEnZwbyZojqvtGPvqZDM71MxaAb8EPnL3pQRh09fMzg87rscS/F2/EL6nLwN3mFlbM0s2s8MbqB6JStRtU1qa30LwzfBPNawfA6ziy47a+QTNDa2q7XciMIXgw3ElwWWnGeG2RcCx1fb/dXictcBtwLvAleG2JODPwDrgC+D7BB+WFm7PIuiXWEZwmesnwLm1/F4/JwidqsuycNtZBJ3Sm8LXHxSuH0rQAb0lrPEFvuw4foigjbwEmA2cuZf39U2CztaOVdadR3AGUEpwdvXXKu/vncCdtRyre1h/SbXlB+H2+8Kffz1cPxHoUeXnDwWmhr/vVODQKtvaAfeH9WwAngnXH7nr/aqy79f+PrU0vWXXfxaRFsHMTiL4cKzetCFVmNl9BB/aP4m6FomemoakWTOzVDM7OWzC6Azcwpcd0yJSBwoCae6MoDN1A0Gzz1yC6/JFpI7UNCQiEud0RiAiEuea3X0EHTp08O7du0ddhohIszJ16tS17p5T07aYBYGZFQAPENzm78B4d7+92j5nEFy/XElw2dx17v7fPR23e/fuFBUVxaZoEZEWysyq3y2+WyzPCMqB69394/Bmoalm9rq7z6myz5vAc+7uFoxA+QTQP4Y1iYhINTHrI3D3le7+cfh4C8HVHJ2r7VPiX/ZW77qDVEREGlGjdBaHY6QcAEyqYdtZZjaPYGjby2v5+XHhuO5FxcXFsSxVRCTuxDwIwsGsniZo/99cfbu7P+vu/QnGr/llTcdw9/HuXujuhTk5NfZ1iIjIfoppEIQDdD1NMOnGM3va190nAj3NrMOe9hMRkYYVsyAIh+qdQDDTUm1T9PUO98PMDiQYynhdrGoSEZGvi+VVQ4cQzHQ0Mxw/HuBmgnHNcfc7gW8CF5tZGcHwvWNdtzqLiDSqmAVBeD+A7WWf3xNM4hFzxVt2cMc787nppAG0StIN1SIiu8TNJ+KUReu59/1F/OTfM9FJh4jIl+ImCE4e0olrjunDE0XLGD9xYdTliIg0Gc1urKH6+P6xfVhYXMLvXplH9w5tOGFQx6hLEhGJXNycEQCYGX88ZxhDu2Rz3WPTmLV8U9QliYhELq6CACAlOZF/XTyCtmnJXHl/Eas3b4+6JBGRSMVdEADkZqQw4dKD2LK9jG89UMS2nRVRlyQiEpm4DAKAAZ0yuf3cA5i5fBPXPzmNykpdSSQi8SlugwDg2IF5/O/JA3hp5ipue/2zqMsREYlEXF01VJMrDu3BguIS/v72fHrmtOEbB3aJuiQRkUYV12cEEFxJdOsZgzm4V3t+/PRMihatj7okEZFGFfdBAJCcmMA/LxhBl7apjHtwKkvWbY26JBGRRqMgCGWlJTPh0oOoqHSuuH8Km7eXRV2SiEijUBBU0aNDG+68cARfrC3l6kc+obyiMuqSRERiTkFQzf/0as+vzhzMxM+K+eULc6IuR0Qk5uL+qqGanDuyKwvXljJ+4kJ65aZz8f90j7okEZGYieUMZQVm9raZzTGz2WZ2bQ37XGBmM8xsppl9YGbDYlXPvrrxxP4cOyCPXzw/h3c/K466HBGRmIll01A5cL27DwRGA981s4HV9vkCOMLdhxBMXD8+hvXsk8QE4/Zzh9M3L4OrH/6Yz1dvibokEZGYiFkQuPtKd/84fLwFmAt0rrbPB+6+IXz6EdCk7uZq0zqJCZcUktIqkcvvn8K6kh1RlyQi0uAapbPYzLoDBwCT9rDbFcDLjVHPvsjPTuXuiwtZs3kH/+/Bqewo1wB1ItKyxDwIzCwdeBq4zt0317LPUQRBcGMt28eZWZGZFRUXN357/bCCbP40ZhhFizdw09Oa6lJEWpaYBoGZJROEwMPu/kwt+wwF7gbOcPd1Ne3j7uPdvdDdC3NycmJX8B6cOjSf64/ryzOfLOeOdxZEUoOISCzE7PJRMzNgAjDX3W+rZZ+uwDPARe7e5If/vPro3iwoLuH/Xv2Unh3acNKQTlGXJCJSb7G8j+AQ4CJgpplNC9fdDHQFcPc7gZ8B7YE7gtyg3N0LY1hTvZgZv/vmUJZu2Mb3n5hG57apDO2SHXVZIiL1Ys2tvbuwsNCLiooirWFtyQ7O/Mf77Cyv5D9XH0KnrNRI6xER2Rszm1rbF20NMbEfOqS35p5LD2LrzgquuK+I0h3lUZckIrLfFAT7qW9eBn87/wDmrdrMdY9rqksRab4UBPVwVL9cfnbqQF6fs5rfvzov6nJERPaLBp2rp0sO7s6C4lLuenchvXLSGVNYEHVJIiL7RGcE9WRm3HLaQA7r04H/fXYmHy2s8VYIEZEmS0HQAJISE/j7+QfSrX0brnpoKovWlkZdkohInSkIGkhWajITLinEgMvvn8KmrZrqUkSaBwVBA+rWvg13XVTI0vVb+c4jUynTVJci0gwoCBrYyB7t+O03hvL+/HXc8txsDVAnIk2erhqKgbNHdGFhcQl3vLOA3jnpXH5oj6hLEhGplYIgRm44vh8Li0v51Ytz6N4hjaP750VdkohIjdQ0FCMJCcZtY4cxKD+L7z3yCfNW1TgVg4hI5BQEMZTWKol/XVxIekoSV9xXRPEWTXUpIk2PgiDGOmalMOGSg1hfupNxDxaxvUxTXYpI06IgaASDO2fx57HD+WTJRn701AxdSSQiTYqCoJGcOLgjN57Yn+emr+D2Nz+PuhwRkd1iFgRmVmBmb5vZHDObbWbX1rBPfzP70Mx2mNkNsaqlqbjqiJ5888Au/OWNz/lgwdqoyxERAWJ7RlAOXO/uA4HRwHfNbGC1fdYD1wB/jGEdTYaZ8aszB9OzQxt++OQMNm/XMBQiEr2YBYG7r3T3j8PHW4C5QOdq+6xx9ylA3HwiprZK5I9jhrFy0zZ++fycqMsREWmcPgIz6w4cAEzaz58fZ2ZFZlZUXFzckKVF4sCubfnOkb15cuoyXp+zOupyRCTOxTwIzCwdeBq4zt33664qdx/v7oXuXpiTk9OwBUbkmmP6MLBTJjc9M4N1Jbq/QESiE9MgMLNkghB42N2fieVrNTetkhL489jhbN5Wzs3PztQlpSISmVheNWTABGCuu98Wq9dpzvp1zOD64/vy6uzVPPvJ8qjLEZE4FctB5w4BLgJmmtm0cN3NQFcAd7/TzDoCRUAmUGlm1wED97cJqTm68rCevDF3Nbc8N5vRPduTn50adUkiEmesuTVJFBYWelFRUdRlNKgl67Zy4u0TOaBrNg9ePoqEBIu6JBFpYcxsqrsX1rRNdxY3AV3bp/GTUwby/vx1PPjR4qjLEZE4oyBoIs4bWcCR/XL47ctzWVhcEnU5IhJHFARNhJnx+28OpXVSIj94Yjrlmu9YRBqJgqAJyctM4VdnDmba0o3c+e6CqMsRkTihIGhiThuWz2nD8vnLG58za/mmqMsRkTigIGiCfnnGINq1acX1T0zXRDYiEnMKgiYoO60Vvz97KJ+u3sKfX/8s6nJEpIVTEDRRR/XL5byRXRn/3kKmLFofdTki0oIpCJqwn5wygIK2afzgiWmU7CiPuhwRaaEUBE1Ym9ZJ/GnMMJZt2MavX5wbdTki0kIpCJq4g7q3Y9xhPXl08hLe/nRN1OWISAukIGgGvn9cX/rlZXDjUzPYULoz6nJEpIVREDQDKcmJ3DZ2GBu27uSn/5kVdTki0sIoCJqJQflZXHtMH16YsZLnpq+IuhwRaUEUBM3IVUf0YnhBNj/99yxWb94edTki0kLEcoayAjN728zmmNlsM7u2hn3MzP5qZvPNbIaZHRirelqCpMQEbhszjB3lFfzoqRma3lJEGkQszwjKgevdfSAwGviumQ2sts9JQJ9wGQf8M4b1tAg9c9K56aQBvPtZMY9OXhp1OSLSAsQsCNx9pbt/HD7eAswFOlfb7QzgAQ98BGSbWadY1dRSXDS6G4f27sCvXpzD4nWlUZcjIs1co/QRmFl34ABgUrVNnYGqX2uX8fWwkGoSEow/nD2UxATj+iemU1GpJiIR2X97DQIz62VmrcPHR5rZNWaWXdcXMLN04Gnguv2dlN7MxplZkZkVFRcX788hWpz87FR+cfogihZv4O73FkZdjog0Y3U5I3gaqDCz3sB4oAB4pC4HN7Pk8OcfdvdnathleXi8XbqE677C3ce7e6G7F+bk5NTlpePCWQd05sRBHfnTa58xb9V+ZayISJ2CoNLdy4GzgL+5+w+Bvbbjm5kBE4C57n5bLbs9B1wcXj00Gtjk7ivrWHvcMzN+fdZgMlOT+P7j09lZruktRWTf1SUIyszsPOAS4IVwXXIdfu4Q4CLgaDObFi4nm9lVZnZVuM9LwEJgPvAv4Dv7Vr60T2/Nb78xlLkrN/PXNz+PuhwRaYaS6rDPZcBVwK/d/Qsz6wE8uLcfcvf/AraXfRz4bl0KldodNzCPc0Z04Y535nP0gFwO7No26pJEpBnZ6xmBu89x92vc/VEzawtkuPvvG6E22Qc/O20gnbJSuf6J6WzbqektRaTu6nLV0Dtmlmlm7YCPgX+ZWW1t/hKRjJRk/u+coXyxtpTfvay5C0Sk7urSR5AVXvb5DYKbv0YBx8a2LNkfB/fqwOWH9OD+Dxfz38/XRl2OiDQTdQmCpPBu3zF82VksTdSPTuxHr5w2/PCp6WzaVhZ1OSLSDNQlCG4FXgUWuPsUM+sJ6PKUJiolOZHbxgxnzZYd/OK52VGXIyLNQF06i59096Hu/u3w+UJ3/2bsS5P9Nawgm6uP6s0znyznlVm6LUNE9qwuncVdzOxZM1sTLk+bWZfGKE7239VH92ZI5yxufnYWxVt2RF2OiDRhdWkaupfgDuD8cHk+XCdNWHI4d0HJjnJuekZzF4hI7eoSBDnufq+7l4fLfYAG/GkG+uRl8KMT+vHG3DU8OXVZ1OWISBNVlyBYZ2YXmlliuFwIrIt1YdIwLj+kB6N6tOPW5+ewbMPWqMsRkSaoLkFwOcGlo6uAlcDZwKUxrEkaUEKC8cdzhuHu3PDkdCo1d4GIVFOXq4YWu/vp7p7j7rnufibwtfmHpekqaJfGLacN4qOF67n3g0VRlyMiTcz+zlA2pkGrkJg7p7ALx/TP5Q+vzGP+mi1RlyMiTcj+BsEeRxWVpsfM+O03h5DWKpEfPDGdsgrNXSAigVqDwMza1bK0R0HQLOVmpPCbs4YwY9km/vH2/KjLEZEmYk/zEUwFnJo/9HfGphyJtZOGdOLM4fn8/a35HN0/l6Fd6jz9tIi0ULWeEbh7D3fvGf5Zfem5twOb2T3hncizatneNrxjeYaZTTazwfX5RaTufnH6YDqkt+bK+4t47/PiqMsRkYjtbx9BXdwHnLiH7TcD09x9KHAxcHsMa5EqstKSufeyg8hMTeaiCZO59fk5bC/TZDYi8SpmQeDuE4H1e9hlIPBWuO88oLuZ5cWqHvmqAZ0yeeF7h3Lpwd255/0vOP3v/2XOis1RlyUiEYjlGcHeTCeY7AYzGwl0A2oczM7MxplZkZkVFRerKaOhpCQn8vPTB3H/5SPZsLWMM//xPuMnLtBNZyJxpk5BYGaHmtll4eOccAL7+vodkG1m04DvAZ8ANbZPuPt4dy9098KcHA1z1NCO6JvDq9cdzlH9c/jNS/M4/+6PWLFxW9RliUgjqcsw1LcANwI3hauSgYfq+8LuvtndL3P34QR9BDnAwvoeV/ZPuzatuPPCEfzh7KHMXLaJE/4ykf9MWx51WSLSCOpyRnAWcDpQCuDuK4CM+r6wmWWbWavw6ZXAxHBuZImImTGmsICXrz2cvnkZXPvYNK559BNNeSnSwtUlCHZ6MJi9A5hZm7oc2MweBT4E+pnZMjO7wsyuMrOrwl0GALPM7FPgJDR+UZPRtX0aj48bzfXH9eXFmSs56S8T+XCBBpwVaalsbxOWmNkNQB/gOOC3BKORPuLuf4t9eV9XWFjoRUVFUbx0XJq2dCPff3wai9aVMu6wnvzg+L60TkqMuiwR2UdmNtXdC2vcVpeZq8zsOOB4gruMX3X31xu2xLpTEDS+rTvL+fWLc3l40hIGdMrkL2OH069jvVsHRaQR1TsImhIFQXTenLuaHz01gy07yvnxif259ODuJCRo2CmR5mBPQVCXq4a2mNnmasvScHiIvQ41IS3HMQPyeOW6wzmsdwdufWEOl9w7mVWbtkddlojUU106i/8C/BDoTHDD1w3AI8BjwD2xK02aopyM1tx9SSG/OWsIRYs2cMJfJvLSzJVRlyUi9VCXIDjd3e9y9y3htf/jgRPc/XGgbYzrkybIzDh/VFdevOZQurdP4zsPf8z1T0xny3ZdZirSHNUlCLaa2RgzSwiXMcCu9oDm1cEgDapnTjpPfftgrjmmD89+soyTbn+PKYv2NLyUiDRFdQmCC4CLgDXA6vDxhWaWClwdw9qkGUhOTOAHx/XlyasOJsGMsXd9yP+9Oo+d5ZoBTaS50FVD0mBKdpRz6/OzeaJoGUM6Z/HnscPpnZsedVkiQj0vHzWzFOAKYBCQsmu9u1/ekEXWlYKg6Xtl1ipuemYG28oq+N+TB3Dh6G6Y6TJTkSjV6/JR4EGgI3AC8C7BlUNbGq48aWlOHNyRV687nJE92vPT/8zmsvumsGaLLjMVaarqEgS93f2nQKm73w+cAoyKbVnS3OVmpnD/ZQfxi9MH8eGCdZz4l/d4bfaqqMsSkRrUJQh2XRO4MZxXOAvIjV1J0lKYGZcc3J0XrzmUTlkpjHtwKj9+egalO8qjLk1EqqhLEIw3s7bAT4DngDnA72NalbQovXMzePY7h/DtI3vxeNFSTv7re3y8ZEPUZYlIaI9BYGYJwGZ33+DuE929p7vnuvtdjVSftBCtkhK48cT+PPat0ZRXOOfc+SF/fv0zyit0malI1PYYBO5eCfyokWqRODCqZ3tevu4wzhiWz+1vfs7Zd37InBWaj0gkSnVpGnrDzG4wswIza7dr2dsPmdk9ZrbGzGbVsj3LzJ43s+lmNnvXnMjS8mWmJHPb2OH8/fwD+GJtKSf/9T3O/ucH/GfacnaU1zhttYjEUF3uI/iihtXu7nscedTMDgdKgAfcfXAN228Gstz9RjPLAT4FOrr7zj0dV/cRtCwbSnfy1NRlPDxpMYvWbaV9m1acU1jABaO6UtAuLeryRFqMPd1HkLS3H3b3Hvvzou4+0cy672kXIMOCO43SgfWALieJM23btOJbh/fkikN78N/5a3noo8WMn7iAuyYu4Ii+OVw4qhtH9c8lUfMeiMTMXoPAzNKAHwBd3X2cmfUB+rn7C/V87b8TXIW0AsgAxoZ9EhKHEhKMw/vmcHjfHFZu2sajk5fy2OQlXPlAEZ2zUzl/VFfGFBaQk9E66lJFWpy6NA09DkwFLnb3wWEwfODuw/d68OCM4IVamobOBg4hCJlewOvAMHf/Ws+hmY0DxgF07dp1xOLFi/f20tIClFVU8sac1Tz40WI+WLCO5ETjhEEduXB0N0b1aKdhK0T2Qb2ahoBe7j7WzM4DcPet1jD/Ay8DfudBEs0P+yL6A5Or7xjOgTAegj6CBnhtaQaSExM4aUgnThrSiQXFJTz80RKemrqUF2aspE9uOheM6so3RnQhMyU56lJFmrW6XDW0Mxxy2gHMrBewowFeewlwTHjMPKAfsLABjistUK+cdH522kAm3Xwsfzh7KGmtEvn583MY9es3uemZGcxavinqEkWarbo0DR0P/C8wEHiNoDnnUnd/Zy8/9yhwJNCBYB6DW4BkAHe/08zygfuAToARnB08tLeCddWQ7DJj2UYe+mgxz01fwfaySoYXZHPh6G6cOrQTKcmJUZcn0qTUaxjq8ADtgdEEH9gfufvahi2x7hQEUt2mrWU8/fEyHpq0mIXFpWSnJXPOiC5cMKob3Tu0ibo8kSahvvMRPE8wWf1z7l4ag/r2iYJAauPufLhwHQ9/tIRXZ6+ivNI5rE8HLhjVjWMH5JKUWJeWUJGWqb5BcAQwlmD46SnAYwRXAkUywLyCQOpizebtPDZlKY9OXsLKTdvpmJnCuSMLOG9kV/IyU/Z+AJEWpt5NQ+FBEoGjgW8BJ7p7ZsOVWHcKAtkX5RWVvDVvDQ9NWsLEz4pJTDCOH5jHhaO7cXCv9roEVeJGfS8fJbxq6DSCM4MDgfsbrjyR2ElKTOD4QR05flBHFq0t5ZHJS3iyaCkvz1pFzw5tOH9UV84ZUUBWmi5BlfhVl6ahJ4CRwCvA48C7Ud4BrDMCqa/tZRW8NHMlD320mI+XbKR1UgKnDcvn7BFdOKh7Ow1nIS1SffsITgDecPeK8PmhwHnu/t0Gr7QOFATSkOas2MxDkxbz70+Ws3VnBTkZrTl5cEdOHZbPiK5tSVAoSAvREJePHgCcB4wBvgCecfe/NWiVdaQgkFjYurOct+at4cUZK3lr3hp2lFeSl9mak4d04tSh+RxQkK1QkGZtv4LAzPoSfPifB6wlaBa6wd27xarQulAQSKyV7CjnzbmreXHGSt75rJid5ZXkZ6Vw8pBOnDK0E8MLstXJLM3O/gZBJfAecIW7zw/XLdzbPASxpiCQxrRlexlvhKHw7mfFlFU4nbNTOXVoEApDOmcpFKRZ2N8gOBM4l2BIiVcI7h+4e3/nJ2goCgKJyqZtZbw+ZzUvzljBe5+vpbzS6doujVOGduKUIZ0YlJ+pUJAmq76dxW2AMwiaiI4GHgCedffXGrrQulAQSFOwcetOXpu9mhdmruT9+WupqHR6dGjDKWHzUf+OGQoFaVIa5Iay8EBtgXMIJpE5poHq2ycKAmlq1pfu5NXZq3hxxko+WLCWSodeOW04ZWg+pw7tRN+8jKhLFGm4IGgKFATSlK0t2cErs4JQmPTFOiod+ualc8qQfE4Z2oneuelRlyhxSkEgEoE1W7bzyqxVvDBjJVMWrccd+nfMCDua8+mhkVGlESkIRCK2evN2Xp65khdmrKRo8QYABnbK5NRhQUdzt/YKBYktBYFIE7Jy0zZemrmKF2as4JMlGwEY0jmLU4d24uQhnSholxZxhdISRRIEZnYPcCqwppbJ638IXBA+TQIGADnuvn5Px1UQSEuybMNWXg5DYfqyTSQY/O6bQxlTWBB1adLCRBUEhwMlwAM1BdAUv2cAAA7ZSURBVEG1fU8Dvu/uR+/tuAoCaamWrt/Kzc/O5P35a7njggM5cXCnqEuSFmRPQRCzKZvcfSKwx2/3VZwHPBqrWkSag4J2adx10QiGF2RzzaPTeO/z4qhLkjgR+dx9ZpYGnAg8vYd9xplZkZkVFRfrP4e0XGmtkrj30pH0zGnDuAemMjXsWBaJpciDgGDCm/f31Dfg7uPdvdDdC3NychqxNJHGl5WWzANXjCQvszWX3TuZuSs3R12StHBNIQjORc1CIl+Rm5HCQ1eOIq1VEhdNmMyitaVRlyQtWKRBYGZZwBHAf6KsQ6Qp6tI2jYeuHEmlOxfcPYlVm7ZHXZK0UDELAjN7FPgQ6Gdmy8zsCjO7ysyuqrLbWcBr7q6vOyI16J2bwf2XjWTTtjIunDCJ9aU7oy5JWiDdUCbSDHy0cB2X3DOZvnkZPPKtUWSkJEddkjQzkVw+KiINZ3TP9txxwYHMXbmZK+8vYntZRdQlSQuiIBBpJo4ZkMefxgxj8qL1fPfhjymrqIy6JGkhFAQizcgZwztz6xmDeXPeGm54cjqVlc2raVeapqSoCxCRfXPR6G5s3lbG/736KZkpydx6xiDNhib1oiAQaYa+c2QvNm8r466JC8lKTeaGE/pFXZI0YwoCkWbIzPjxSf3ZvL2Mv789n8zUJMYd3ivqsqSZUhCINFNmxq/OHMLm7eX85qV5ZKYkc+7IrlGXJc2QgkCkGUtMMP48Zjgl28u56dmZZKQkc8pQDV8t+0ZXDYk0c62SErjzwhGM6NqW6x7/hHc+XRN1SdLMKAhEWoDUVolMuPQg+uRmcNVDUylaVNepQEQUBCItRlZqMHx1flYql903hdkrNkVdkjQTCgKRFqRDemsevHIUGa2TuHjCZBYWl0RdkjQDCgKRFqZzdioPXjkKgAvvnsSKjdsirkiaOgWBSAvUKyed+y8fyZbt5Vw4YRJrS3ZEXZI0YQoCkRZqcOcsJlx6EMs3bOOSeyazeXtZ1CVJExXLiWnuMbM1ZjZrD/scaWbTzGy2mb0bq1pE4tXIHu2488IRfLpqC1feV8S2nRq+Wr4ulmcE9wEn1rbRzLKBO4DT3X0QcE4MaxGJW0f1z+XPY4czZfF6vvPwVHaWa/hq+aqYBYG7TwT2dDHz+cAz7r4k3F93wYjEyGnD8vn1mUN4+9NifvDENCo0fLVUEeUQE32BZDN7B8gAbnf3ByKsR6RFO39UVzZvL+N3L88jIyWZ35w1WMNXCxBtECQBI4BjgFTgQzP7yN0/q76jmY0DxgF07apBtUT211VH9GLTtjL++c4CslKT+fFJ/aMuSZqAKINgGbDO3UuBUjObCAwDvhYE7j4eGA/B5PWNWqVIC/OjE/qxeVsZd74bhMG3j9Tw1fEuystH/wMcamZJZpYGjALmRliPSFwwM249YzCnDcvn96/M4+FJi6MuSSIWszMCM3sUOBLoYGbLgFuAZAB3v9Pd55rZK8AMoBK4291rvdRURBpOYoJx25hhlGwv4yf/nkVGSjKnD8uPuiyJiLk3r5aWwsJCLyoqiroMkRZh284KLrlnMh8v2cC/Li7kqP65UZckMWJmU929sKZturNYJI6ltkrk7ksL6d8pGL560sJ1UZckEVAQiMS5zJRk7r9sJF3apnLl/UXMWq7hq+ONgkBEaJ/emgevGEVmajIX3zOZ+Ws0fHU8URCICAD52ak8dOUoEgzG3vUhv3xhDh8uWEd5hYakaOnUWSwiX/Hpqi387uW5vL9gHTvLK8lKTeaofjkcMyCPI/rlkJmSHHWJsh/21FmsIBCRGpXuKOe9z9fyxtzVvDVvDetLd5KUYIzu2Z5jB+RyzIA8CtqlRV2m1JGCQETqpaLS+WTJBt6Yu4Y35q7e3YfQv2MGxw3M49gBeQzpnEVCgsYuaqoUBCLSoL5YW8qbc1fz+pzVTFm0nkqHnIzWHDsgl2MH5HFI7w6kJCdGXaZUoSAQkZjZULqTdz5bwxtz1vDuZ8WU7CgnJTmBw/rkcNyAPI7qn0tORuuoy4x7CgIRaRQ7yyuZ9MU63pizmjfmrmH5xm2YwfCCbI4dkMdxA/Pok5uu4a8joCAQkUbn7sxduYU35q7mjbmrmbEsuFGta7s0jh2Qx7EDczmoezuSE3UVe2NQEIhI5FZv3s6bYWfzf+evZWd5JZkpSRzVP7gC6Yi+OWSl6tLUWFEQiEiTsnVneGnqnODS1HXhpamjerYLzhZ0aWqDUxCISJNVUelMW7qB1+d8/dLUI/rmcEDXbIYVZNMxM0V9C/WgIBCRZmPR2lLeCC9N/XjJBsoqgs+o3IzWDCvIZnhBNsO6ZDOkS5aakvaBgkBEmqUd5RXMXbmF6Us3Mn3pRqYt28jC4tLd23vmtGF4l+CMYVhBNgM6ZdA6Sfcv1GRPQRDLGcruAU4F1rj74Bq2H0kwXeUX4apn3P3WWNUjIs1P66REhodnAbts2lbGzGWbmL5sI58s2cjEz9fyzCfLAUhONAZ2ygyCIQyInh3a6I7nvYjZGYGZHQ6UAA/sIQhucPdT9+W4OiMQkarcnZWbtu8+Y5i+dCMzl22idGcFABmtkxhakLW7SWl4QTa5mSkRV934IjkjcPeJZtY9VscXEQEwM/KzU8nPTuWkIZ2AoAN6QXEJ08ImpenLNnLXuwsprwy++HbKStl9xjCsIIshnbPIiONRVWMWBHX0P2Y2HVhBcHYwu6adzGwcMA6ga9eujVieiDRHiQlG37wM+uZlMKawAIDtZRXMXrF5dzBMX7qRV2avAsAMeuek7+5rGN4lm34dM2iVFB83u8W0szg8I3ihlqahTKDS3UvM7GTgdnfvs7djqmlIRBrKhtKdYShs2h0O60p3AtAqKYFB+ZkM65LNoPxM+uZl0Ds3nTato/7+vH8iaRraG3ffXOXxS2Z2h5l1cPe1UdUkIvGlbZtWHNkvlyP75QJBf8OyDdt2h8L0pZt4fMpStpVV7P6Zztmp9MlL3x0Mu/5Mb6YBAREGgZl1BFa7u5vZSIJpM9dFVY+IiJlR0C6NgnZpnDo0H4DyikqWrN/KZ6tLmL9mC5+tLuHzNSV8EM7gtkvn7NQwGNLpk5dBn9zgz+YQELG8fPRR4Eigg5ktA24BkgHc/U7gbODbZlYObAPO9eZ2U4OItHhJiQn0zEmnZ0460HH3+vKKSpZu2MZnq7cwf00Jn63ewuerS/hw4VcDIj8rZXcw9M3LoHdeOn1y05tU57RuKBMRaUAVlc7S9VuDYFhTwufhn/PXlLCjSkB0+kpApNM7N4M+eekxmxO6SfYRiIi0RIkJRvcObejeoQ3HD/pyfUWls2zD1rBpKTh7+HzNFh6etI7tZV8GRMfMlN19ELual2IZEKAgEBFpFIkJRrf2bejWvg3HDczbvb6i0lkeNjFVPYN4ZNKSr3RSd8xM4crDenDlYT0bvDYFgYhIhBITjK7t0+jaPo1jqwREZaWzfOOXAfHZ6i0xm/JTQSAi0gQlJHx5BdMxA/L2/gP1ea2YHl1ERJo8BYGISJxTEIiIxDkFgYhInFMQiIjEOQWBiEicUxCIiMQ5BYGISJxrdoPOmVkxsHg/f7wDoPkOvqT346v0fnxJ78VXtYT3o5u759S0odkFQX2YWVFto+/FI70fX6X340t6L76qpb8fahoSEYlzCgIRkTgXb0EwPuoCmhi9H1+l9+NLei++qkW/H3HVRyAiIl8Xb2cEIiJSjYJARCTOxU0QmNmJZvapmc03sx9HXU+UzKzAzN42szlmNtvMro26pqiZWaKZfWJmL0RdS9TMLNvMnjKzeWY218z+J+qaomJm3w//j8wys0fNLCXqmmIhLoLAzBKBfwAnAQOB88xsYLRVRaocuN7dBwKjge/G+fsBcC0wN+oimojbgVfcvT8wjDh9X8ysM3ANUOjug4FE4Nxoq4qNuAgCYCQw390XuvtO4DHgjIhrioy7r3T3j8PHWwj+o3eOtqromFkX4BTg7qhriZqZZQGHAxMA3H2nu2+MtqpIJQGpZpYEpAErIq4nJuIlCDoDS6s8X0Ycf/BVZWbdgQOASdFWEqm/AD8CKqMupAnoARQD94ZNZXebWZuoi4qCuy8H/ggsAVYCm9z9tWirio14CQKpgZmlA08D17n75qjriYKZnQqscfepUdfSRCQBBwL/dPcDgFIgLvvUzKwtQctBDyAfaGNmF0ZbVWzESxAsBwqqPO8SrotbZpZMEAIPu/szUdcToUOA081sEUGT4dFm9lC0JUVqGbDM3XedIT5FEAzx6FjgC3cvdvcy4Bng4Ihriol4CYIpQB8z62FmrQg6fJ6LuKbImJkRtAHPdffboq4nSu5+k7t3cffuBP8u3nL3Fvmtry7cfRWw1Mz6hauOAeZEWFKUlgCjzSwt/D9zDC204zwp6gIag7uXm9nVwKsEPf/3uPvsiMuK0iHARcBMM5sWrrvZ3V+KsCZpOr4HPBx+aVoIXBZxPZFw90lm9hTwMcGVdp/QQoea0BATIiJxLl6ahkREpBYKAhGROKcgEBGJcwoCEZE4pyAQEYlzCgKRasyswsymVVka7M5aM+tuZrMa6ngiDSEu7iMQ2Ufb3H141EWINBadEYjUkZktMrM/mNlMM5tsZr3D9d3N7C0zm2Fmb5pZ13B9npk9a2bTw2XX8ASJZvavcJz718wsNbJfSgQFgUhNUqs1DY2tsm2Tuw8B/k4wainA34D73X0o8DDw13D9X4F33X0YwXg9u+5m7wP8w90HARuBb8b49xHZI91ZLFKNmZW4e3oN6xcBR7v7wnDQvlXu3t7M1gKd3L0sXL/S3TuYWTHQxd13VDlGd+B1d+8TPr8RSHb3X8X+NxOpmc4IRPaN1/J4X+yo8rgC9dVJxBQEIvtmbJU/Pwwff8CXUxheALwXPn4T+DbsnhM5q7GKFNkX+iYi8nWpVUZlhWD+3l2XkLY1sxkE3+rPC9d9j2BGrx8SzO61a7TOa4HxZnYFwTf/bxPMdCXSpKiPQKSOwj6CQndfG3UtIg1JTUMiInFOZwQiInFOZwQiInFOQSAiEucUBCIicU5BICIS5xQEIiJx7v8DORnxEqve1x8AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Plot:\n",
        "\n",
        "plt.plot(range(0, len(NN_losses)), NN_losses)\n",
        "plt.title(\"Average Loss vs. Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CSl47RKppHM"
      },
      "source": [
        "### (c) (5 points) Report the final testing accuracy of trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "j56JIxgGabx4"
      },
      "outputs": [],
      "source": [
        "## Code:\n",
        "\n",
        "def Test_Accuracy_NN(model, test_data_loader):\n",
        "\n",
        "  iters = 0\n",
        "  correct = 0\n",
        "\n",
        "  for images, labels in test_data_loader:\n",
        "\n",
        "    images = images.view(100, -1)\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    labels = labels.float()\n",
        "    y_predict = model(images)\n",
        "     ## We take the output neuron with the highest predicted value\n",
        "    max_index, y_pred_max = torch.max(y_predict, dim = 1)\n",
        "    iters += labels.size(0)\n",
        "    correct += (y_pred_max == labels).sum().item()\n",
        "\n",
        "  accuracy = correct / iters\n",
        "  print(f\"Test Accuracy is: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71u05hUhO8gY",
        "outputId": "c3135d48-e77f-4034-ac5a-55286d24fd2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy is:  0.4821\n"
          ]
        }
      ],
      "source": [
        "Test_Accuracy_NN(NN_mod, test_loader_cifar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbHsQrUyqNCR"
      },
      "source": [
        "### 2. (30 points) Implement a 7 layers CNN with 4 convolutional layers, 3 fully-connected layers and ReLU activation function. The input dimension of the 1st fully-connected layer must be 4096."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Uy7Tb8W8qUMr"
      },
      "outputs": [],
      "source": [
        "## Implementation of CNN\n",
        "## You can insert more code chunks and text cells if you want to.\n",
        "## Your code:\n",
        "\n",
        "# We know images is torch.Size([63,3,32,32])\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 3)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = 3)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.pool3 = nn.MaxPool2d(3,3)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3)\n",
        "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)\n",
        "        self.fc1 = nn.Linear(4*32*32, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (F.relu(self.conv1(x)))\n",
        "        x = (F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = (F.relu(self.conv3(x)))  \n",
        "        x = self.pool3(F.relu(self.conv4(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Z5BSajajVohg"
      },
      "outputs": [],
      "source": [
        "def hype_tune_CNN(num_epochs, lr, batch_size, train_loader_cifar = train_loader_cifar):\n",
        "  print(\"\")\n",
        "  print(\" >>>> Number of Epochs: \", num_epochs, \" Learning Rate: \", lr, \" Batch Size: \", batch_size, \"<<<<\")\n",
        "  print(\"\")\n",
        "\n",
        "  ### Instantiating the model\n",
        "  model_CNN = CNN()\n",
        "  model_CNN.to(device)\n",
        "\n",
        "  ### I am going to use the Binary Cross Entropy Loss for Logistic\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  iter = 0\n",
        "  losses = list()\n",
        "  optimizer = optim.SGD(model_CNN.parameters(),lr=lr)\n",
        "  print(\"Starting for loop\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      epoch_losses = list()\n",
        "\n",
        "      for i, (images, labels) in enumerate(train_loader_cifar):\n",
        "\n",
        "          ## TODO \n",
        "\n",
        "          # 1. Compute Loss. Check torch functions for the corresponding loss for Logistic and SVM\n",
        "          # print(i, images.size())\n",
        "          # images = images.view(100, -1)\n",
        "\n",
        "          # print(images.size())\n",
        "          # print(device)\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          # print(images.device)\n",
        "          # print(labels.device)\n",
        "          # print(model_NN.device)\n",
        "          optimizer.zero_grad()\n",
        "          y_predict = model_CNN(images)\n",
        "          loss = criterion(y_predict, labels)\n",
        "          \n",
        "          # 2. Do optimization. Chec torch.optim to see how to do optimization with pytorch\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          iter += 1\n",
        "\n",
        "          # 3. Save batch loss\n",
        "          epoch_losses.append(loss.item()) \n",
        "\n",
        "      ## Save average epoch loss\n",
        "        # if (iter) % 64 == 0:\n",
        "      losses.append(np.average(epoch_losses))\n",
        "      print('Epoch: {} :: Iteration: {} :: Loss: {}  '.format(epoch, iter, losses[-1], ))\n",
        "\n",
        "\n",
        "  return losses, model_CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJfUBA2lhWrJ",
        "outputId": "1ff3ae79-1f4d-442f-fdd8-f52ad34a9be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  100 <<<<\n",
            "\n",
            "Starting for loop\n",
            "Epoch: 0 :: Iteration: 500 :: Loss: 2.1812491252422332  \n",
            "Epoch: 1 :: Iteration: 1000 :: Loss: 1.895059068918228  \n",
            "Epoch: 2 :: Iteration: 1500 :: Loss: 1.7144573636054992  \n",
            "Epoch: 3 :: Iteration: 2000 :: Loss: 1.5593539731502533  \n",
            "Epoch: 4 :: Iteration: 2500 :: Loss: 1.421438115119934  \n",
            "Epoch: 5 :: Iteration: 3000 :: Loss: 1.3020122748613359  \n",
            "Epoch: 6 :: Iteration: 3500 :: Loss: 1.2190733706951142  \n",
            "Epoch: 7 :: Iteration: 4000 :: Loss: 1.1493120149374008  \n",
            "Epoch: 8 :: Iteration: 4500 :: Loss: 1.0921173218488693  \n",
            "Epoch: 9 :: Iteration: 5000 :: Loss: 1.0433399212360381  \n"
          ]
        }
      ],
      "source": [
        "CNN_losses, CNN_mod = hype_tune_CNN(10, 0.1, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvRutMHMJKt9",
        "outputId": "83c62494-bcd5-43f1-d000-0962d4f0a79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.01  Batch Size:  100 <<<<\n",
            "\n",
            "Starting for loop\n",
            "Epoch: 0 :: Iteration: 500 :: Loss: 2.2944551019668578  \n",
            "Epoch: 1 :: Iteration: 1000 :: Loss: 2.160778523683548  \n",
            "Epoch: 2 :: Iteration: 1500 :: Loss: 2.013424638271332  \n",
            "Epoch: 3 :: Iteration: 2000 :: Loss: 1.953555480480194  \n",
            "Epoch: 4 :: Iteration: 2500 :: Loss: 1.90576544713974  \n",
            "Epoch: 5 :: Iteration: 3000 :: Loss: 1.856375496864319  \n",
            "Epoch: 6 :: Iteration: 3500 :: Loss: 1.806465538263321  \n",
            "Epoch: 7 :: Iteration: 4000 :: Loss: 1.7455555057525636  \n",
            "Epoch: 8 :: Iteration: 4500 :: Loss: 1.6956315703392029  \n",
            "Epoch: 9 :: Iteration: 5000 :: Loss: 1.6646231994628906  \n"
          ]
        }
      ],
      "source": [
        "CNN_losses2, CNN_mod2 = hype_tune_CNN(10, 0.01, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFpLEy6lJOaj"
      },
      "source": [
        "Again, we see that between the two sets of hyperparameters I chose here, the model with the lower loss after 10 epochs is the one with the higher learnning rate of 0.1 (CNN_mod) instead of 0.01."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X_eNkjVqUtJ"
      },
      "source": [
        "### (a) (5 points) Print the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uauSCPLpa5zp",
        "outputId": "5e3e4a8d-aead-487f-96ed-d4a739de9cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (pool3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout): Dropout(p=0.15, inplace=False)\n",
            "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=4096, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "## Print the model:\n",
        "\n",
        "print(CNN())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YsS3aAwa7m5"
      },
      "source": [
        "### (b) (10 points) Report the **Average loss of an epoch** for every epoch by generating Average Loss vs. Epoch plot. Please report at least **10** epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vHO3ftYObA8O",
        "outputId": "bd6f30d5-1ed7-434a-8087-5b221569a85e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Plot:\n",
        "\n",
        "plt.plot(range(0, len(CNN_losses)), CNN_losses)\n",
        "plt.title(\"Average Loss vs. Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb3abqoabCc1"
      },
      "source": [
        "### (c) (5 points) Report the final testing accuracy of trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "L7_rnkvdbEWN"
      },
      "outputs": [],
      "source": [
        "## Code:\n",
        "\n",
        "def Test_Accuracy_CNN(CNN_mod = CNN_mod, test_loader_cifar = test_loader_cifar):\n",
        "\n",
        "  #Since we're doing prediction, we don't need to calculate the gradient\n",
        "  with torch.no_grad():\n",
        "\n",
        "    iters = 0\n",
        "    correct = 0\n",
        "\n",
        "    for images, labels in test_loader_cifar:\n",
        "\n",
        "      # images = images.view(100, -1)\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      labels = labels.float()\n",
        "      y_predict = CNN_mod(images)\n",
        "      ## We take the output neuron with the highest predicted value\n",
        "      max_index, y_pred_max = torch.max(y_predict, dim = 1)\n",
        "      iters += labels.size(0)\n",
        "      correct += (y_pred_max == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / iters\n",
        "    print(f\"Test Accuracy is: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMnPSNoLIGrJ",
        "outputId": "4e1e3caa-a820-4ae9-af53-2d019c851054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy is:  0.6848\n"
          ]
        }
      ],
      "source": [
        "Test_Accuracy_CNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1tfLFPRbFYZ"
      },
      "source": [
        "### (d) (10 points) Write a new cifar$\\_$loaders function to try different data augmentation methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck8sXvXqbH4C",
        "outputId": "a73f18a3-6d93-40b7-c734-7d9528dc9b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "## Code:\n",
        "\n",
        "\n",
        "# Here I will define a few different sets of transforms that could be used to augment the data. \n",
        "\n",
        "from torchvision.transforms import Compose \n",
        "\n",
        "train_transform_1 = Compose([ \n",
        "    transforms.RandomVerticalFlip(p=0.75),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
        "])\n",
        "\n",
        "train_transform_2 = Compose([ transforms.RandomPerspective(75),\n",
        "                             #This randomly inverts pixels above a certain value\n",
        "                             transforms.RandomSolarize(0.1, 0.5),\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
        "])\n",
        "\n",
        "\n",
        "def cifar_loaders_transform(batch_size, transformations, shuffle_test=False): \n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.225, 0.225, 0.225])\n",
        "    \n",
        "    ### Here is where you can use different transforms!\n",
        "    train = datasets.CIFAR10(data_dir, train=True, download=True, \n",
        "        transform=transformations)\n",
        "    \n",
        "    test = datasets.CIFAR10(data_dir, train=False, \n",
        "        transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0, 0, 0], [1, 1, 1])]))\n",
        "    \n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
        "        shuffle=True, pin_memory=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
        "        shuffle=shuffle_test, pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader_cifar2, test_loader_cifar2 = cifar_loaders_transform(batch_size_cifar, train_transform_1)\n",
        "train_loader_cifar3, test_loader_cifar3 = cifar_loaders_transform(batch_size_cifar, train_transform_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i_2hrQfFdcr",
        "outputId": "f45bfefa-8989-4f24-a3e3-0dd9aaa33e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  100 <<<<\n",
            "\n",
            "Starting for loop\n",
            "Epoch: 0 :: Iteration: 500 :: Loss: 2.3003194789886474  \n",
            "Epoch: 1 :: Iteration: 1000 :: Loss: 2.138669214248657  \n",
            "Epoch: 2 :: Iteration: 1500 :: Loss: 1.9141116499900819  \n",
            "Epoch: 3 :: Iteration: 2000 :: Loss: 1.7911286563873292  \n",
            "Epoch: 4 :: Iteration: 2500 :: Loss: 1.6527294149398803  \n",
            "Epoch: 5 :: Iteration: 3000 :: Loss: 1.5085359511375427  \n",
            "Epoch: 6 :: Iteration: 3500 :: Loss: 1.4187828249931336  \n",
            "Epoch: 7 :: Iteration: 4000 :: Loss: 1.3219720336198806  \n",
            "Epoch: 8 :: Iteration: 4500 :: Loss: 1.2539794957637787  \n",
            "Epoch: 9 :: Iteration: 5000 :: Loss: 1.1802804691791535  \n"
          ]
        }
      ],
      "source": [
        "CNN_losses2, CNN_mod2 = hype_tune_CNN(10, 0.1, 100, train_loader_cifar2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCBBDc60IXQc",
        "outputId": "bf4f6b61-6f2a-4067-b102-6d2c6c50826f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy is:  0.5087\n"
          ]
        }
      ],
      "source": [
        "Test_Accuracy_CNN(CNN_mod2, test_loader_cifar2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaD5NLL-Gnge",
        "outputId": "109671cf-dadc-408f-909a-48c1c27bbe88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  100 <<<<\n",
            "\n",
            "Starting for loop\n",
            "Epoch: 0 :: Iteration: 500 :: Loss: 2.3030995383262636  \n",
            "Epoch: 1 :: Iteration: 1000 :: Loss: 2.3028348479270937  \n",
            "Epoch: 2 :: Iteration: 1500 :: Loss: 2.302688429355621  \n",
            "Epoch: 3 :: Iteration: 2000 :: Loss: 2.301919527053833  \n",
            "Epoch: 4 :: Iteration: 2500 :: Loss: 2.298336494922638  \n",
            "Epoch: 5 :: Iteration: 3000 :: Loss: 2.2896360516548158  \n",
            "Epoch: 6 :: Iteration: 3500 :: Loss: 2.2638634276390075  \n",
            "Epoch: 7 :: Iteration: 4000 :: Loss: 2.185349744319916  \n",
            "Epoch: 8 :: Iteration: 4500 :: Loss: 2.126018327951431  \n",
            "Epoch: 9 :: Iteration: 5000 :: Loss: 2.092679036617279  \n"
          ]
        }
      ],
      "source": [
        "CNN_losses3, CNN_mod3 = hype_tune_CNN(10, 0.1, 100, train_loader_cifar3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaOLcC6iIZRi",
        "outputId": "6659d28d-f48a-4dc0-ebbf-8126f5ef08e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy is:  0.348\n"
          ]
        }
      ],
      "source": [
        "Test_Accuracy_CNN(CNN_mod3, test_loader_cifar3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFc_zYNRJL2L",
        "outputId": "135dae0d-97e1-4fe0-9bb5-e08311fb3b7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  100 <<<<\n",
            "\n",
            "Starting for loop\n",
            "Epoch: 0 :: Iteration: 500 :: Loss: 2.3015249142646788  \n",
            "Epoch: 1 :: Iteration: 1000 :: Loss: 2.210594249486923  \n",
            "Epoch: 2 :: Iteration: 1500 :: Loss: 2.0808105742931366  \n",
            "Epoch: 3 :: Iteration: 2000 :: Loss: 1.9768878843784332  \n",
            "Epoch: 4 :: Iteration: 2500 :: Loss: 1.8953319025039672  \n",
            "Epoch: 5 :: Iteration: 3000 :: Loss: 1.8401395168304444  \n",
            "Epoch: 6 :: Iteration: 3500 :: Loss: 1.793049777507782  \n",
            "Epoch: 7 :: Iteration: 4000 :: Loss: 1.7479873497486114  \n",
            "Epoch: 8 :: Iteration: 4500 :: Loss: 1.71213317155838  \n",
            "Epoch: 9 :: Iteration: 5000 :: Loss: 1.6792094604969026  \n"
          ]
        }
      ],
      "source": [
        "NN_losses2, NN_mod2 = hype_tune_NN(10, 0.1, 100, train_loader_cifar2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_SXudfsJWLj",
        "outputId": "1f63f495-7454-4086-fb84-b90559e2e7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy is:  0.3599\n"
          ]
        }
      ],
      "source": [
        "Test_Accuracy_NN(NN_mod2, test_loader_cifar2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYII5kZsJ2cd",
        "outputId": "9547b13a-c6d1-4a4f-e28e-e4dddd5d5d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " >>>> Number of Epochs:  10  Learning Rate:  0.1  Batch Size:  100 <<<<\n",
            "\n",
            "Starting for loop\n",
            "Epoch: 0 :: Iteration: 500 :: Loss: 2.3026629786491393  \n",
            "Epoch: 1 :: Iteration: 1000 :: Loss: 2.302585841178894  \n",
            "Epoch: 2 :: Iteration: 1500 :: Loss: 2.3025858402252197  \n",
            "Epoch: 3 :: Iteration: 2000 :: Loss: 2.3025858306884768  \n",
            "Epoch: 4 :: Iteration: 2500 :: Loss: 2.3025858402252197  \n",
            "Epoch: 5 :: Iteration: 3000 :: Loss: 2.3025858402252197  \n",
            "Epoch: 6 :: Iteration: 3500 :: Loss: 2.302585841178894  \n",
            "Epoch: 7 :: Iteration: 4000 :: Loss: 2.302585841178894  \n",
            "Epoch: 8 :: Iteration: 4500 :: Loss: 2.3025858402252197  \n",
            "Epoch: 9 :: Iteration: 5000 :: Loss: 2.3025858402252197  \n"
          ]
        }
      ],
      "source": [
        "NN_losses3, NN_mod3 = hype_tune_NN(10, 0.1, 100, train_loader_cifar3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEc7TpsyKx4r"
      },
      "source": [
        "There is some interesting behavior in the loss function here -- it seems that a local minimum or some other type of \"rut\" is reached and the descent function doesn't move any further. This might be interesting to look into further at a later point, but for now won't change the ain conclusion we'll draw here about the relative improved performance of CNN compared to NN on transformed or augmented data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PRwv5_HKHj1",
        "outputId": "54addc4f-43eb-479d-dde1-f096f067ab4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy is:  0.1\n"
          ]
        }
      ],
      "source": [
        "Test_Accuracy_NN(NN_mod3, test_loader_cifar3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uUForcfbI1W"
      },
      "source": [
        "\n",
        "Answer: Above I ran both the MLP and the CNN on the augmented data using the set of transformations in 'train_transform_1' as above. The second set of transforms I created resulted in very low accuracy for both MLP (0.1) and CNN (0.348) when compared to the first set of tranforms I created (which had accuracy of 0.3599 for MLP and 0.5087 for CNN, respectively). However, all of these accuracies were still lower than that which I achieved with the set of transforms given in the original loaders in the homework. This exercise demonstrates the importance of data augmentation in the overall performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a0igZ32oqsh"
      },
      "source": [
        "I used this website to learn more about data augmentation for CNN and MLPs: \n",
        "\n",
        "https://medium.com/swlh/how-data-augmentation-improves-your-cnn-performance-an-experiment-in-pytorch-and-torchvision-e5fb36d038fb\n",
        "\n",
        "https://www.tutorialspoint.com/pytorch-torchvision-transforms-gaussianblur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF01LpJsbXSU"
      },
      "source": [
        "### 3. (10 points) Please compare the results of the two models (MLP and CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpTZ4mm9bZxn"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Overall, the CNN performed better than MLP on the data set with the transformations provided in the homework. Over the course of the homework, I tried several different combinations of optimizers and hyper-parameters. In every circumstance, including the final hyper-parameters that I selected and reported here, the fully connected neural network had a lower overall accuracy (0.4821) on the test data than the convolutional neural network (0.6848). \n",
        "\n",
        "When I used my own transforms, both the MLP and the CNN performed more poorly overall. In all cases, however, the CNN performs better than the MLP, even with the transforms that seemed to diminish the behavior of all models.\n",
        "\n",
        "It makes sense that the CNN would perform better on image data because it is designed to pick up on image-related features (i.e. edges) through it's kernels and pooling layers, which serve to reduce dimensionality. There are certain tasks which a fully-connected network might perform better on than a CNN, but according to the resources I found on the web, it's typical for a CNN to outperform a full-connected NN when image data is being used.\n",
        "\n",
        "I referenced this website in writing my answer: https://medium.datadriveninvestor.com/why-are-convolutional-neural-networks-good-for-image-classification-146ec6e865e8"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
